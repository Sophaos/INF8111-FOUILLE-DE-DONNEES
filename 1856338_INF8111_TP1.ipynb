{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vapGWIBgFt0q"
      },
      "source": [
        "# INF8111 - Fouille de données\n",
        "\n",
        "## TP1 Automne 2020 - Duplicate Bug Report Detection\n",
        "\n",
        "##### Membres de l'équipe / Team members:\n",
        "\n",
        "    - Son-Thang Pham (1856338)\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgkUX6Ki0W9s"
      },
      "source": [
        "## 1 - Résumé / Overview\n",
        "\n",
        "À cause de la complexité des systèmes informatiques, les bogues logiciels sont courants. Les entreprises, et en particulier les plus grosses, utilisent un *Bug Tracking System* (BTS), aussi appelé *Issue Tracking System*, pour organiser et suivre les rapports de bogues. En plus des développeurs et des testeurs, de nombreux projets (notamment ceux libres de droits) permettent aux utilisateurs de soumettre un rapport de bogues dans leur BTS. Pour ce faire, les utilisateurs doivent remplir un formulaire avec plusieurs champs. La majorité de ces champs contient des données catégoriques et accepte uniquement des valeurs prédéfinies (composant, version du produit et du système, etc.). Deux autres champs importants sont le résumé ou *summary* en anglais, et la description. Les utilisateurs sont libres d’écrire ce qu’ils veulent dans ces deux champs avec pour seule contrainte le nombre de caractères. La soumission de ces champs crée une page que l’on appelle rapport de bogue et qui contient toute l’information à propos du bogue.\n",
        "\n",
        "Par manque de communication et de synchronisation, les utilisateurs ne savent pas toujours qu’un bogue a déjà été soumis et peuvent donc le soumettre à nouveau. Identifier les rapports qui correspondent au même bogue (duplicata) est une tache importante des BTSs et est le sujet de ce TP. Notre objectif est de développer un système qui comparera les nouveaux rapports de bogues avec ceux déjà soumis en fonction de leur similarité textuelle. La liste triée des rapports les plus similaires sera utilisée par un opérateur humain pour identifier manuellement si un rapport est un duplicata.\n",
        "\n",
        "---\n",
        "\n",
        "Due to the complexity of software systems, software bugs are prevalent. Companies, especially the larger ones, usually use a Bug Tracking System (BTS), also called Issue Tracking System, to manage and track records of bugs. Besides developers and testers, many projects, mainly open source projects, allow users to report new bugs in their BTS.\n",
        "To do that, users have to fill out a form with multiple fields. An important subset of\n",
        "these fields provides categorical data and only accepts values that range from a fixed list of\n",
        "options (e.g. component, version and product of the system). Two other important fields\n",
        "are the summary and the description. The users are free to write anything in both fields\n",
        "with the only constraint that the summary has a maximum number of characters. The\n",
        "submission of a form creates a page, called bug report or issue report which contains all\n",
        "the information about a bug.\n",
        "\n",
        "Due to the lack of communication and synchronization, users may not be aware that a specific bug was already submitted and may report it again. Identifying duplicate bug reports is an important task in the BTSs and is the subject of this TP. Our objective is to develop a system that will compare a new bug report with the already submitted ones and rank them based on textual similarity. This ranked list will be used by a triager to manually identify whether a report is a duplicate or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFsBsrFzFt0r"
      },
      "source": [
        "# 2 Installation / Setup\n",
        "\n",
        "Pour ce TP, vous aurez besoin des librairies `numpy`, `sklearn` et `scipy` (que vous avez sans doute déjà), ainsi que la librairie `nltk`, qui est une libraire utilisée pour faire du traitement du language (Natural Language Processing, NLP). Installez les libraires en question et exécutez le code ci-dessous :\n",
        "\n",
        "---\n",
        "\n",
        "For this assignment, you need the `numpy`, `sklearn` and `scipy` libraries (which you may already have), as well as the `nltk` library, which is used for Natural Language Processing (NLP). Please run the code below to install the packages needed for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8r8BtgdFt0u",
        "outputId": "31393590-b144-4672-a499-96a832843e81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "                                                                                                                                                                                                                # If you want, you can use anaconda and install after nltk library\n",
        "# pip install --user numpy\n",
        "# pip install --user sklearn\n",
        "# pip install --user scipy\n",
        "# pip install --user nltk\n",
        "# pip install --user tqdm\n",
        "\n",
        "\n",
        "#python\n",
        "import nltk\n",
        "import re\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\sonth\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\sonth\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGq_fsnCFt02"
      },
      "source": [
        "## 3 - Jeu de données / Data\n",
        "\n",
        "Téléchargez l'archive à l'adresse suivante: https://drive.google.com/file/d/14BrFaOiVDIcpjwpm5iAkAy0v_moZyGiP/view?usp=sharing\n",
        "\n",
        "In this zip file, there are: \n",
        "\n",
        "L'archive contient:\n",
        "1. test.json: Ce fichier contient les duplicate bug reports that will be used to evaluate our system.\n",
        "2. threads: Ce dossier contient le code HTML des bug report. Chaque fichier HTML est nommé selon le motif **bug_report_id.html**.\n",
        "\n",
        "\n",
        "L'image ci-dessous illustre un exemple de bug report:\n",
        "\n",
        "![https://ibb.co/tqyRM4L](bug_report.png)\n",
        "\n",
        "- A : identifiant du bug report\n",
        "- B : date de création\n",
        "- C : résumé\n",
        "- D : composant\n",
        "- E : produit\n",
        "- F : l'identifiant du rapport dont le bug report est dupliqué\n",
        "- G : description\n",
        "\n",
        "\n",
        "Le script suivant charge le jeu de données de test et définit certaines variables globales:\n",
        "\n",
        "---\n",
        "\n",
        "Please download the zip file at the following adress: https://drive.google.com/file/d/14BrFaOiVDIcpjwpm5iAkAy0v_moZyGiP/view?usp=sharing\n",
        "\n",
        "This archive contains: \n",
        "\n",
        "1. test.json: This file contains duplicate bug reports that will be used to evaluate our system.\n",
        "2. bug_reports: It is a folder that contains the bug report html source. Each html file name follows the pattern **bug_report_id.html**.\n",
        "\n",
        "\n",
        "Figure below depicts an bug report page example:\n",
        "\n",
        "![https://ibb.co/tqyRM4L](bug_report.png)\n",
        "\n",
        "\n",
        "- A : bug report id\n",
        "- B : creation date\n",
        "- C : summary\n",
        "- D : component\n",
        "- E : product\n",
        "- F : the report id which the bug report is duplicate\n",
        "- G : description\n",
        "\n",
        " The following script loads the test dataset and define some global variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdRjO-NbFt04"
      },
      "source": [
        "import os\n",
        "\n",
        "# define the folder path that contain the data\n",
        "# FOLDER_PATH = \"Define folder path that contain threads folder and test.json\"\n",
        "#FOLDER_PATH = \"dataset/\"\n",
        "FOLDER_PATH = \"dataset/\"\n",
        "PAGE_FOLDER = os.path.join(FOLDER_PATH, 'bug_reports')\n",
        "\n",
        "\n",
        "# Load the evaluation dataset\n",
        "import json\n",
        "\n",
        "\n",
        "test = json.load(open(os.path.join(FOLDER_PATH, \"test.json\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1WXzfKOFt08"
      },
      "source": [
        "# 4 - Web scraping\n",
        "\n",
        "\"Le *web scraping* (parfois appelé harvesting) est une technique d'extraction du contenu de sites Web, via un script ou un programme, dans le but de le transformer pour permettre son utilisation dans un autre contexte, par exemple le référencement.\" [Wikipedia](https://fr.wikipedia.org/wiki/Web_scraping)\n",
        "\n",
        "---\n",
        "\n",
        "*Web scraping* also called harvesting consists in extracting relevant data from web pages and prepare it for computational analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyWZeHkdFt08"
      },
      "source": [
        "## 4.1 - Question 1 (2 points)\n",
        "\n",
        "Implémentez *extract_data_from_page*. Cette fonction extrait l'information suivante du code HTML : identifiant du rapport de bogue, date de création, titre, produit, composant, identifiant du rapport de bogue bug dont il est un duplicata, résumé et description. \n",
        "\n",
        "\n",
        "La fonction *extract_data_from_page* retourne un dictionnaire avec la structure suivante :\n",
        "```\n",
        " {\"report_id\": int, \n",
        "  \"dup_id\": int or None (the report id which it is duplicate), \n",
        "  \"component\": string, \n",
        "  \"product\": string, \n",
        "  \"summary\": str, \n",
        "  \"description\": string, \n",
        "  \"creation_date\": int} \n",
        "```\n",
        "\n",
        "Par exemple, quand la fonction *extract_data_from_page* reçoit le rapport \"bug_report/7526.html\", elle retourne :\n",
        "\n",
        "```\n",
        "{\n",
        "\"report_id\": 7526,\n",
        "\"dup_id\": 7799,\n",
        "\"product\": \"core graveyard\",\n",
        "\"component\":  tracking,\n",
        "\"summary\": \"Apprunner crashes on exit\",\n",
        "\"description\": \"Apprunner crashes on exit, without fail. The browser window closes, but the\n",
        "console window hangs around. I end up having to kill winoldap with the \\\\\"Close\n",
        "Program\\\\\" dialog (Ctrl-Alt-Del).\",\n",
        "\"creation_date\": 928396140\n",
        "}\n",
        "```\n",
        "\n",
        "**La date de création doit être représentée comme un timestamp (entier). Si un bug n'est pas un duplicata, alors dup_id doit être None.**\n",
        "\n",
        "*Indice: lxml parse est plus rapide que html.parser*\n",
        "\n",
        "---\n",
        "\n",
        "Implement *extract_data_from_page*. This function extracts the following information from the html: bug report id, creation date, title, product, component, the report id which it is duplicate, summary and description.\n",
        "\n",
        "The *extract_data_from_page* function returns a dictionary with the following structure:\n",
        "```\n",
        " {\"report_id\": int, \n",
        "  \"dup_id\": int or None (the report id which it is duplicate), \n",
        "  \"component\": string, \n",
        "  \"product\": string, \n",
        "  \"summary\": str, \n",
        "  \"description\": string, \n",
        "  \"creation_date\": int} \n",
        "```\n",
        "\n",
        "For instance, when extract_data_from_page receives \"bug_report/7526.html\", it returns:\n",
        "\n",
        "```\n",
        "{\n",
        "\"report_id\": 7526,\n",
        "\"dup_id\": 7799,\n",
        "\"product\": \"core graveyard\",\n",
        "\"component\":  tracking,\n",
        "\"summary\": \"Apprunner crashes on exit\",\n",
        "\"description\": \"Apprunner crashes on exit, without fail. The browser window closes, but the\n",
        "console window hangs around. I end up having to kill winoldap with the \\\\\"Close\n",
        "Program\\\\\" dialog (Ctrl-Alt-Del).\",\n",
        "\"creation_date\": 928396140\n",
        "}\n",
        "```\n",
        "\n",
        "**Creation date have to be represented as timestamp. If bug report is not duplicate, dup_id have to be None.**\n",
        "\n",
        "*HINT: lxml parse is faster than html.parser*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKv3_jACFt0_"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_data_from_page(pagepath):\n",
        "    \"\"\"\n",
        "    Scrap bug report content from bug report html.\n",
        "    \n",
        "    :param pagepath: the path of html file.\n",
        "    :return: \n",
        "        {\n",
        "        \"report_id\": int,\n",
        "        \"dup_id\": int or None (the report id which it is duplicate), \n",
        "        \"component\": string, \n",
        "        \"product\": string, \n",
        "        \"summary\": str, \n",
        "        \"description\": string, \n",
        "        \"creation_date\": int\n",
        "        }\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Only remove new lines from the component and product.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(open(pagepath,\"r\", encoding='utf8'), \"lxml\")\n",
        "    data = {}\n",
        "    data['report_id'] = int(re.sub('[^0-9]','', soup.find(id=\"field-value-bug_id\").text.strip()))\n",
        "    dup_id = re.sub('[^0-9]','', soup.find(id=\"field-value-status-view\").text.strip())\n",
        "    if dup_id:\n",
        "        dup_id = int(dup_id)\n",
        "    else:\n",
        "        dup_id = None\n",
        "    data['dup_id'] = dup_id\n",
        "    data['product'] = soup.find(id=\"product-name\").text.strip().split('\\n')[0]\n",
        "    data['component'] = soup.find(id=\"component-name\").text.strip().split('\\n')[0]\n",
        "    data['summary'] = soup.find(id=\"field-value-short_desc\").text.strip()\n",
        "    #data['description'] = re.sub('\\s+',' ',soup.find(id=\"ct-0\").text.strip())\n",
        "    data['description'] = soup.find(id=\"ct-0\").text.strip()\n",
        "    data['creation_date'] = int(soup.find(\"span\", class_ = \"bug-time-label\").find(\"span\", class_ = \"rel-time\")[\"data-time\"])\n",
        "\n",
        "    return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESIo2sXt73fa",
        "outputId": "0f03d5e3-82cf-4b33-87a8-67650d6a9b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "extract_data_from_page(r\"dataset\\bug_reports\\7526.html\") # 41 no duplicate\n",
        "#extract_data_from_page(r\"dataset\\bug_reports\\41.html\") # 41 no duplicate\n",
        "#extract_data_from_page(r\"dataset\\bug_reports\\2920.html\") # 2920"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'report_id': 7526,\n",
              " 'dup_id': 7799,\n",
              " 'product': 'Core Graveyard',\n",
              " 'component': 'Tracking',\n",
              " 'summary': 'Apprunner crashes on exit',\n",
              " 'description': 'Apprunner crashes on exit, without fail. The browser window closes, but the\\nconsole window hangs around. I end up having to kill winoldap with the \"Close\\nProgram\" dialog (Ctrl-Alt-Del).',\n",
              " 'creation_date': 928381771}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU7f5OIzFt1E"
      },
      "source": [
        "## 4.3 - Extraire le texte du code HTML / Extract text from HTML\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w8BxGjgFt1E",
        "scrolled": true
      },
      "source": [
        "import os\n",
        "from multiprocessing import Pool, TimeoutError\n",
        "from time import time\n",
        "import json\n",
        "import tqdm\n",
        "\n",
        "# Index each thread by its id\n",
        "index_path = os.path.join(PAGE_FOLDER, 'bug_reports.json')\n",
        "\n",
        "if os.path.isfile(index_path):\n",
        "    # Load threads that webpage content were already extracted.\n",
        "    report_index = json.load(open(index_path))\n",
        "else:\n",
        "    # Extract webpage content\n",
        "\n",
        "    # This can be slow (around 10 minutes). Test your code with a small sample. lxml parse is faster than html.parser\n",
        "    files = [os.path.join(PAGE_FOLDER, filename) for filename in os.listdir(PAGE_FOLDER)]\n",
        "    reports = [extract_data_from_page(f) for f in tqdm.tqdm(files)]\n",
        "    report_index = dict(((report['report_id'], report) for report in reports ))\n",
        "\n",
        "    # Save preprocessed bug reports\n",
        "    json.dump(report_index, open(index_path,'w'))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pd5Sx79Ft1G"
      },
      "source": [
        "# 5 - Prétraitement des données / Data Preprocessing\n",
        "\n",
        "Le prétraitement des données est une tache cruciale en fouille de données. Cette étape nettoie et transforme les données brutes dans un format qui permet leur analyse, et leur utilisation avec des algorithmes de *machine learning*. En traitement des langages (natural language processing, NLP), la *tokenization* et le *stemming* sont des étapes cruciales. De plus, vous implémenterez une étape supplémentaire pour filtrer les mots sans importance.\n",
        "\n",
        "---\n",
        "\n",
        "Preprocessing is a crucial task in data mining. The objective is to clean and transform the raw data in a format that is better suited for data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well-known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
        "\n",
        "## 5.1 - Tokenization\n",
        "\n",
        "Cette étape permet de séparer un texte en séquence de *tokens* (= jetons, ici des mots, symboles ou ponctuation). Par example, la phrase *\"It's the student's notebook.\"* peut être séparé en liste de tokens de cette manière: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n",
        "\n",
        "---\n",
        "\n",
        "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuation). For instance, the sentence *\"It's the student's notebook.\"* can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
        "\n",
        "\n",
        "### 5.1.1 - Question 2 (0.5 points) \n",
        "Implémentez la fonction suivante :\n",
        "\n",
        "- **tokenize_space** qui tokenize le texte à partir des blancs (espace, tabulation, nouvelle ligne). Ce tokenizer est naïf.\n",
        "- **tokenize_nltk** qui utilise le tokenizer par défaut de la librairie nltk (https://www.nltk.org/api/nltk.html).\n",
        "- **tokenize_space_punk** replace la ponctuation par des espaces puis tokenize les tokens qui sont séparés par des blancs (espace, tabulation, retour à la ligne).\n",
        "\n",
        "**Tous les tokenizers doivent mettre les tokens en minuscule.**\n",
        "\n",
        "---\n",
        "\n",
        "Implement the following functions: \n",
        "- **tokenize_space** tokenizes the tokens that are separated by whitespace (space, tab, newline). This is a naive tokenizer.\n",
        "- **tokenize_nltk** uses the default method of the nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
        "- **tokenize_space_punk** replaces the punctuation to space and then tokenizes the tokens that are separated by whitespace (space, tab, newline).\n",
        "\n",
        "**All tokenizers have to lowercase the tokens.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zM6Yq1pFt1H"
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def tokenize_space(text):\n",
        "    \"\"\"\n",
        "    Tokenize the tokens that are separated by whitespace (space, tab, newline). \n",
        "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
        "    \n",
        "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
        "    \"\"\"\n",
        "\n",
        "    return text.lower().split()\n",
        "        \n",
        "def tokenize_nltk(text):\n",
        "    \"\"\"\n",
        "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
        "    \"\"\"\n",
        "\n",
        "    return nltk.word_tokenize(text.lower())\n",
        "\n",
        "def tokenize_space_punk(text):\n",
        "    \"\"\"\n",
        "    This tokenizer replaces punctuation to spaces and then tokenizes the tokens that are separated by whitespace (space, tab, newline).\n",
        "    \"\"\"\n",
        "\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    return nltk.word_tokenize(regex.sub(' ', text.lower()))\n",
        "    \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYfbumna0BeG",
        "outputId": "8f329091-3481-43b3-939e-48e4ecc67259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(tokenize_space(\"Hello\\tworld of\\nNLP!!\"))\n",
        "print(tokenize_nltk(\"Hello\\tworld of\\nNLP!!\"))\n",
        "print(tokenize_space_punk(\"Hello\\tworld of\\nNLP!!\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'world', 'of', 'nlp!!']\n",
            "['hello', 'world', 'of', 'nlp', '!', '!']\n",
            "['hello', 'world', 'of', 'nlp']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpFMlhFTFt1J"
      },
      "source": [
        "## 5.2 - Suppression des mots vides / Stop words removal\n",
        "\n",
        "### 5.2.1 - Question 3 (0.5 point)\n",
        "\n",
        "Certains tokens sont sans importance pour la comparaison, car ils apparaissent dans la majorité des discussions. Les supprimer réduit la dimension du vecteur et accélère les calculs.\n",
        "\n",
        "Expliquez quels tokens sont sans importances pour la comparaison des discussions. Implémentez la fonction filter_tokens qui retire ces mots de la liste des tokens.\n",
        "\n",
        "---\n",
        "\n",
        "There are a set of tokens that are not significant to the similarity comparison since they appear in most of bug report pages. Thus, removing them decreases the vector dimensionality and turns the similarity calculation computationally cheaper.\n",
        "\n",
        "Describe the tokens that can be removed without affecting the similarity comparison? Moreover, implement the function filter_tokens that removes these words from a list of tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usdvFGFHFt1J"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def filter_tokens(tokens):\n",
        "    \n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "  return [t for t in tokens if not t in stop_words]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWBJYGaeZjHx",
        "outputId": "7d7ffd14-45cb-4aa7-a801-c893f8f03025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "print(filter_tokens(['hello', 'world', 'of', 'NLP']))\n",
        "print(filter_tokens(tokenize_space_punk(\"This is a sample sentence, showing off the stop words filtration.\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'world', 'NLP']\n",
            "['sample', 'sentence', 'showing', 'stop', 'words', 'filtration']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AITFRrPRFt1L"
      },
      "source": [
        "## 5.3 - Racinisation / Stemming\n",
        "\n",
        "La racinisation, ou *stemming* en anglais, est un procédé de transformation des flexions en leur radical ou racine. Par exemple, en anglais, la racinisation de \"fishing\", \"fished\" and \"fish\" donne \"fish\" (stem). \n",
        "\n",
        "---\n",
        "\n",
        "The process to convert tokens with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*. For instance, the word \"fishing\", \"fished\" , \"fish\" and \"fisher\" are reduced to the stem \"fish\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAY10IY_Ft1L",
        "outputId": "c08cf7ea-9568-4949-fc3a-c075139acab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "\n",
        "word1 = ['I', 'tried', 'different', 'fishes']\n",
        "\n",
        "print([ stemmer.stem(w) for w in word1])\n",
        "\n",
        "word2 = ['I', 'will', 'tries', 'only', 'one', 'fishing']\n",
        "print([ stemmer.stem(w) for w in word2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'tri', 'differ', 'fish']\n",
            "['i', 'will', 'tri', 'onli', 'one', 'fish']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnm9AJPPFt1M"
      },
      "source": [
        "### 5.3.1 - Question 4 (0.5 point) \n",
        "\n",
        "Expliquez comment et pourquoi le stemming est utile à notre système.\n",
        "\n",
        "---\n",
        "\n",
        "Explain how stemming can benefit our system?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMHrsLC-Ft1N"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgnJBXzxFt1O"
      },
      "source": [
        "A MODIFIER!!!!!!!!!!!!!!!!!\n",
        "The stemming is useful for our comparison since by looking at two subjects, we can find two words which have been written in different ways but they have the same root such as: fish and peach. So to distinguish if two words have the same meaning or the same root, we apply stemming to directly compare the words by their roots. In other words, Stemming can help our search engine through reducing the vocabulary and therefore reduce the search time. It helps focusing on the sense of a text instead of its deeper meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqwI0Y3OFt1O"
      },
      "source": [
        "# 6 - Représentation des données / Data Representation\n",
        "\n",
        "# 6.1 - Bag of Words\n",
        "\n",
        "De nombreux algorithmes demandent des entrées qui soient toutes de la même taille, ce qui n'est forcément le cas pour des types de données comme les textes, qui peuvent avoir un nombre variable de mots.  \n",
        "\n",
        "Par exemple, considérons la phrase 1, ”Board games are much better than video games” et la phrase 2, ”Monopoly is an awesome game!”. La table ci-dessous montre un exemple d'un moyen de représentation de ces deux phrases en utilisant une représentation fixe : \n",
        "\n",
        "|<i></i>     | an | are | ! | Monopoly | awesome | better | games | than | video | much | board | is | game |\n",
        "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
        "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
        "| Sentence 2 | 1  | 0   | 1 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
        "\n",
        "Chaque colonne représente un mot du vocabulaire (de longueur 13), tandis que chaque ligne contient l'occurrence des mots dans une phrase. Ainsi, la valeur 2 à la position (1,7) est due au fait que le mot *\"games\"* apparaît deux fois dans la phrase 1. \n",
        "\n",
        "Ainsi, chaque ligne étant de longueur 13, on peut les utiliser comme vecteur pour représenter les phrases 1 et 2. Ainsi, c'est cette méthode que l'on appelle *Bag-of-Words* : c'est une représentation de documents par des vecteurs dont la dimension est égale à la taille du vocabulaire, et qui est construite en comptant le nombre d'occurrences de chaque mot. Ainsi, chaque token est ici associé à une dimension.\n",
        "\n",
        "---\n",
        "\n",
        "Many algorithms only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two tweets: ”Board games are much better than video games” and ”Monopoly is an awesome game!”. These sentences are respectively named as Sentences 1 and 2. The table below depicts how we could represent both sentences using a fixed representation.\n",
        "\n",
        "|            | an | are | ! | monopoly | awesome | better | games | than | video | much | board | is | game |\n",
        "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
        "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
        "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
        "\n",
        "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
        "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
        "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
        "could use those vectors to represent the sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e. an integer.\n",
        "\n",
        "<!-- Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
        "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
        "weight of these words will be much bigger than the other ones which can give them more\n",
        "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
        "those with high frequency) appears in most of the documents and, therefore, they do not\n",
        "help to discriminate documents. For instance, the word *of* appears in a significant\n",
        "number of tweets. Thus, having the word *of* does not make\n",
        "documents more or less similar. However, the word *terrible* is rarer and documents that\n",
        "have this word are more likely to be negative. TF-IDF is a technique that overcomes the word frequency disadvantages. -->\n",
        "\n",
        "### 6.1.2 - Question 5 (2 points)\n",
        "\n",
        "\n",
        "Implémentez le Bag-of-Words en pondérant le vecteur par la fréquence de chaque mot.\n",
        "\n",
        "**Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe [sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html) de scipy.**\n",
        "\n",
        "---\n",
        "\n",
        "Implement the bag-of-words model that weights the vector with the absolute word frequency.\n",
        "\n",
        "**For this exercise, you cannot use any external python library (e.g. scikit-learn). However, if you have a problem with memory size, you can use the scipy class [sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html).**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx5wlODhrUTu"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "     \n",
        "def transform_count_bow(X):\n",
        "    \"\"\"\n",
        "    This method relates each token to a specific integer and  \n",
        "    transforms the text in a vector. Vectors are weighted using the token frequencies in the sentence.\n",
        "\n",
        "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
        "\n",
        "    :return: vector representation of each document\n",
        "    \"\"\"     \n",
        "    data = []\n",
        "    row = []\n",
        "    col = []\n",
        "    vocab= {}\n",
        "    for index,doc in enumerate(X):\n",
        "      words = {}\n",
        "      for word in doc:\n",
        "          vocab.setdefault(word, len(vocab)) # Ok\n",
        "          if word in words:\n",
        "              words[word] += 1\n",
        "          else:\n",
        "              words.setdefault(word, 1)\n",
        "              col.append(vocab[word])\n",
        "              row.append(index)\n",
        "      data.extend(words.values())\n",
        "    return csr_matrix((data, (row, col)), dtype=np.uint8).toarray()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMnV590UZbPJ",
        "outputId": "df59963c-e17d-412d-ac92-641b483e9c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# test\n",
        "X = [['I','will', 'be', 'back', '.'],\n",
        "     ['Helllo', 'world', '!'],\n",
        "     ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']\n",
        "    ]\n",
        "\n",
        "Y = [['Hello', 'world', 'world'],\n",
        "     ['world', 'world', 'world', 'world','data','data','data','data','data','mining','mining','mining','mining','mining','mining'],\n",
        "     ['Hello', 'world', '!', '!', '!','mining']]\n",
        "Z = [[\"coucou\", \"papa\", \"!\"], \n",
        "     [\"coucou\", \"maman\", \"!\"],\n",
        "     [\"coucou\", \"papa\", \"!\" ,\"salut\",\"la\" ,\"compagnie\",\"ca\",\"va\",\"?\"]\n",
        "     ]\n",
        "bow1 = transform_count_bow(X)\n",
        "bow2 = transform_count_bow(Y)\n",
        "bow3 = transform_count_bow(Z)\n",
        "print(bow1)\n",
        "print(bow2)\n",
        "print(bow3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]]\n",
            "[[1 2 0 0 0]\n",
            " [0 4 5 6 0]\n",
            " [1 1 0 1 3]]\n",
            "[[1 1 1 0 0 0 0 0 0 0]\n",
            " [1 0 1 1 0 0 0 0 0 0]\n",
            " [1 1 1 0 1 1 1 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbp7EwyYFt1R"
      },
      "source": [
        "##### 6.2 - TF-IDF\n",
        "\n",
        "L'utilisation de la fréquence d'apparition brute des mots, comme c'est le cas avec le bag-of-words, peut être problématique. En effet, peu de tokens auront une fréquence très élevée dans un document, et de ce fait, le poids de ces mots sera beaucoup plus grand que les autres, ce qui aura tendance à biaiser l'ensemble des poids. De plus, les mots qui apparaissent dans la plupart des documents n'aident pas à les discriminer. Par exemple, le mot \"*de*\" apparaît dans beaucoup de documents de la base de données, et pour autant, avoir ce mot en commun ne permet pas de conclure que des documents sont similaires. Au contraire, le mot \"*génial*\" est plus rare, mais les documents qui contiennent ce mot sont plus susceptibles d'être positifs. TF-IDF est donc une méthode qui permet de pallier à ce problème.\n",
        "\n",
        "TF-IDF pondère le vecteur en utilisant une fréquence de document inverse (IDF) et une fréquence de termes (TF).\n",
        "\n",
        "TF est l'information locale sur l'importance qu'a un mot dans un document donné, tandis que IDF mesure la capacité de discrimination des mots dans un jeu de données. \n",
        "\n",
        "L'IDF d'un mot se calcule de la façon suivante:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\text{idf}_i = \\log\\left( \\frac{N}{\\text{df}_i} \\right),\n",
        "\\end{equation}\n",
        "\n",
        "Avec $N$ le nombre de documents dans la base de données, et $\\text{df}_i$ le nombre de documents qui contiennent le mot $i$.\n",
        "\n",
        "Le nouveau poids $w_{ij}$ d'un mot $i$ dans un document $j$ peut ensuite être calculé de la façon suivante:\n",
        "\n",
        "\\begin{equation}\n",
        "  w_{ij} = \\text{tf}_{ij} \\times \\text{idf}_i,\n",
        "\\end{equation}\n",
        "\n",
        "avec $\\text{tf}_{ij}$ la fréquence du mot $i$ dans le document $j$\n",
        "\n",
        "---\n",
        "\n",
        "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
        "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
        "weight of these words will be much bigger than the other ones which can give them more\n",
        "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
        "those with high frequency) appears in most of the documents and, therefore, they do not\n",
        "help to discriminate documents. For instance, the word *of* appears in a significant\n",
        "number of tweets. Thus, having the word *of* does not make\n",
        "documents more or less similar. However, the word *aeroplane* is rarer and documents that\n",
        "have this word are more likely to be similar. TF-IDF is a technique that overcomes the word frequency disadvantages.\n",
        "\n",
        "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
        "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors. \n",
        "The following equation computes the word IDF:\n",
        "\\begin{equation}\n",
        "  idf_i = \\log\\left( \\frac{N}{df_i} \\right),\n",
        "\\end{equation}\n",
        "Where $N$ is the number of documents in the dataset $df_i$ is the number of documents that contain a word $i$.\n",
        "The new weight $w_{ij}$ of a word $i$ in a document $j$ using TF-IDF is computed as:\n",
        "\\begin{equation}\n",
        "  w_{ij} = tf_{ij} \\times idf_i,\n",
        "\\end{equation}\n",
        "Where $tf_{ij}$ is the term frequency of words $i$ in the document $j$.\n",
        "\n",
        "\n",
        "\n",
        "### 6.2.1 - Question 6 (2.5 points)\n",
        "\n",
        "Implémentez le bag-of-words avec la pondération de TF-IDF\n",
        "\n",
        "**Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe [sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html) de scipy.**\n",
        "\n",
        "---\n",
        "\n",
        "Implement a bag-of-words model that weights the vector using TF-IDF.\n",
        "\n",
        "**For this exercise, you cannot use any external python library (e.g. scikit-learn). However, if you have a problem with memory size, you can use the scipy class [sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHjwqgeLd7-d"
      },
      "source": [
        "def transform_tf_idf_bow(X):\n",
        "    \"\"\"\n",
        "    This method calculates the IDF and TF and \n",
        "    transforms the text in vectors. Vectors are weighted using TF-IDF method.\n",
        "\n",
        "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
        "\n",
        "    :return: vector representation of each document\n",
        "    \"\"\"\n",
        "    N = len(X)\n",
        "    #M = []\n",
        "    data = []\n",
        "    row = []\n",
        "    col = []\n",
        "\n",
        "    vocab= {}\n",
        "    df = {}\n",
        "    idf = {}\n",
        "    for index,doc in enumerate(X):\n",
        "      words = {}\n",
        "      for word in doc:\n",
        "          vocab.setdefault(word, len(vocab)) # Ok\n",
        "          try:\n",
        "            df[word].add(index)\n",
        "          except:\n",
        "            df[word] = {index}\n",
        "          if word in words:\n",
        "            words[word] += 1\n",
        "          else:\n",
        "            words.setdefault(word, 1)\n",
        "            col.append(vocab[word])\n",
        "            row.append(index)\n",
        "      #M.append([i / len(doc) for i in list(words.values())])\n",
        "      data.extend([i / len(doc) for i in list(words.values())])\n",
        "    for word in df:\n",
        "      df[word] = len(df[word])\n",
        "    for word in df:\n",
        "      idf[word] = np.log2(N / df[word])\n",
        "    IDF = list(idf.values())\n",
        "    #print(M)\n",
        "    M = csr_matrix((data, (row, col)), dtype=np.float) \n",
        "    W = np.array(M.toarray()) * np.array(IDF)\n",
        "\n",
        "    #print(np.array(M.toarray()))\n",
        "    #print(np.array(IDF))\n",
        "    #print(W)\n",
        "    #print(M.toarray())\n",
        "    return W\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYfiqiOmenyH",
        "outputId": "9be6d281-e98b-47e0-80ac-e8d48e4a8610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# test\n",
        "X = [['I','will', 'be', 'back', '.'],\n",
        "     ['Helllo', 'world', '!'],\n",
        "     ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']\n",
        "    ]\n",
        "\n",
        "Y = [['Hello', 'world', 'world'],\n",
        "     ['world', 'world', 'world', 'world','data','data','data','data','data','mining','mining','mining','mining','mining','mining'],\n",
        "     ['Hello', 'world', '!', '!', '!','mining']]\n",
        "\n",
        "\n",
        "\n",
        "Z = [[\"It\",\"is\", \"going\",\"to\", \"rain\", \"today\"],\n",
        "     [\"Today\", \"I\", \"am\", \"not\", \"going\", \"outside\"],\n",
        "     [\"I\",\"am\", \"going\",\"to\",\"watch\", \"the\", \"season\", \"premiere\"]]\n",
        "bow1 = transform_tf_idf_bow(Z)\n",
        "print(bow1)\n",
        "#bow2 = transform_tf_idf_bow(Y)\n",
        "#bow3 = transform_tf_idf_bow(Z)\n",
        "#print(bow3)\n",
        "#print(bow2)\n",
        "#print(bow3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.26416042 0.26416042 0.         0.09749375 0.26416042 0.26416042\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.26416042 0.09749375 0.09749375 0.26416042 0.26416042 0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.07312031 0.         0.\n",
            "  0.         0.07312031 0.07312031 0.         0.         0.19812031\n",
            "  0.19812031 0.19812031 0.19812031]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTPmhqzlFt1U"
      },
      "source": [
        "# 7 - Notre système / Our System\n",
        "\n",
        "## 7.1 - Question 7 (1 point)\n",
        "\n",
        "La *pipeline* est la séquence d'étapes de prétraitement des données qui transforme les données brutes dans un format qui permet leur analyse. Pour notre problème, implémentez un pipeline composé des étapes suivantes :\n",
        "\n",
        "1. Concatène le texte du résumé et description\n",
        "2. Tokenize le texte, retire les stop words et stem le tokens. \n",
        "3. Génère la représentation vectorielle avec transform_tf_idf_bow ou transform_count_bow.\n",
        "4. Encode données catégorielles (produit et composant) en entier \n",
        "\n",
        "La pipeline (fonction nlp_pipeline) prend en entrée la liste des rapports de bogues (liste des dictionnaires qui contiennent les informations des rapports), le type de tokenization, le type de vectorizer, un booléen qui active ou désactive la suppression des tokens inutiles et un booléen qui active ou désactive le stemming. La fonction nlp_pipeline retourne un tuple ($p$, $c$, $M$) :\n",
        "- $p$ est le vecteur qui contient l'identifiant des produits des rapports de bogues\n",
        "- $c$ est le vecteur qui contient l'identifiant des composants des rapports de bogues\n",
        "- $M$ est une matrice avec la représentation du texte.\n",
        "\n",
        "Le i-ème élément de $p$, $c$ et $M$ sont le produit, composant et la représentation du texte du i-ème rapport de bogue.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The pipeline is a sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. For our problem, you have to implement a pipeline that:\n",
        "\n",
        "1. Concatenate the summary and description\n",
        "2. Perform the tokenization, stop word removal and stemming in textual data\n",
        "3. Generate the vector representation using transform_tf_idf_bow or transform_count_bow\n",
        "4. Encode the categorical data (the component and product) to integers\n",
        "\n",
        "\n",
        "The pipeline (nlp_pipeline function) receives a list of bug reports (dictionary that contains the report information), tokenization type, vectorizer type, a flag that enables or disable the insignificant token removal and a flag that turn stemming on or off. The nlp_pipeline function returns a tuple ($p$, $c$, $M$):\n",
        "- $p$ is a vector that contains the product values of the bug reports\n",
        "- $c$ is a vector that contains the component values of the bug reports\n",
        "- $M$ is a matrix with the text representation.\n",
        "\n",
        "The $i$-th element of $p$, $c$ and $M$ are the product, component and text representation of the $i$-th report in bug_reports, respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeI39WT6Ft1U"
      },
      "source": [
        "def nlp_pipeline(bug_reports, tokenization_type, vectorizer_type, enable_stop_words, enable_stemming):\n",
        "    \"\"\"\n",
        "    Preprocess and vectorize the threads.\n",
        "    \n",
        "    bug_reports: list of all bug reports([dict()]).\n",
        "    tokenization_type: two possible values \"space_tokenization\" and \"nltk_tokenization\".\n",
        "                            - space_tokenization: tokenize_space function is used to tokenize.\n",
        "                            - nltk_tokenization: tokenize_nltk function is used to tokenize.\n",
        "                            - space_punk_tokenization: tokenize_space_punk is used to tokenize.\n",
        "                            \n",
        "    vectorizer_type: two possible values \"count\" and \"tf_idf\".\n",
        "                            - count: use transform_count_bow to vectorize the text\n",
        "                            - tf_idf: use transform_tf_idf_bow to vectorize the text\n",
        "                            \n",
        "    enable_stop_words: Enables or disables the insignificant stop words removal\n",
        "    enable_stemming: Enables or disables steming\n",
        "    \n",
        "    return: tuple ($p$, $c$, $M$)\n",
        "    \"\"\"\n",
        "    ####pass\n",
        "    productIndexDict = {}\n",
        "    componentIndexDict = {}\n",
        "    p = []\n",
        "    c = []\n",
        "    M = []\n",
        "    # 1. Concatenate the summary and description\n",
        "    for bug in bug_reports.values():\n",
        "        bug_concat = bug[\"summary\"] + \" \" + bug[\"description\"]\n",
        "        \n",
        "        # 2. Perform the tokenization, stop word removal and stemming in textual data\n",
        "        tokens = []\n",
        "        if tokenization_type == \"space_tokenization\":\n",
        "            tokens = tokenize_space(bug_concat)\n",
        "        elif tokenization_type == \"nltk_tokenization\":\n",
        "            tokens = tokenize_nltk(bug_concat)\n",
        "        elif tokenization_type == \"space_punk_tokenization\":\n",
        "            tokens = tokenize_space_punk(bug_concat)\n",
        "\n",
        "        matrix = []\n",
        "        # 3. Generate the vector representation using transform_tf_idf_bow or transform_count_bow\n",
        "        if vectorizer_type == \"count\":\n",
        "            matrix = transform_count_bow(tokens)\n",
        "        elif vectorizer_type == \"tf_idf\":\n",
        "            matrix = transform_tf_idf_bow(tokens)\n",
        "        M.append(matrix)\n",
        "\n",
        "        # 3.1. Filter the insignificant tokens\n",
        "        if enable_stop_words:\n",
        "            tokens = filter_tokens(tokens)\n",
        "\n",
        "        # 3.2. Stem the tokens\n",
        "        if enable_stemming:\n",
        "            tokens = [ stemmer.stem(token) for token in tokens]\n",
        "        \n",
        "        # 4.Encode the categorical data (the component and product) to integers\n",
        "        productIndexDict.setdefault(bug[\"product\"], len(productIndexDict))\n",
        "        p.append(productIndexDict[bug[\"product\"]])\n",
        "        \n",
        "        componentIndexDict.setdefault(bug[\"component\"], len(componentIndexDict))\n",
        "        c.append(componentIndexDict[bug[\"component\"]])\n",
        "    #print(productIndexDict)\n",
        "    #print(componentIndexDict)\n",
        "    return (p, c, M)\n",
        "    \"\"\"\n",
        "    You are right. There is an error in the question 7.1.\n",
        "    M is the vector representation of the textual data and not the text representation.\n",
        "    \"\"\"\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo71tQK4dNNX",
        "outputId": "98ce9b92-be24-4728-b9b0-ad498983b81a"
      },
      "source": [
        "test = nlp_pipeline(bug_reports=report_index,\n",
        "                   tokenization_type=\"nltk_tokenization\",\n",
        "                   vectorizer_type=\"count\",\n",
        "                   enable_stop_words=True,\n",
        "                   enable_stemming=True)\n",
        "#print(test[0])\n",
        "#print(\"----\")\n",
        "#print(test[1])\n",
        "#print(\"----\")\n",
        "#print(test[2])\n",
        "\n",
        "\"\"\"\n",
        "tokenization_type: two possible values \"space_tokenization\" and \"nltk_tokenization\".\n",
        "                            - space_tokenization: tokenize_space function is used to tokenize.\n",
        "                            - nltk_tokenization: tokenize_nltk function is used to tokenize.\n",
        "                            - space_punk_tokenization: tokenize_space_punk is used to tokenize.\n",
        "                            \n",
        "vectorizer_type: two possible values \"count\" and \"tf_idf\".\n",
        "                            - count: use transform_count_bow to vectorize the text\n",
        "                            - tf_idf: use transform_tf_idf_bow to vectorize the text\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntokenization_type: two possible values \"space_tokenization\" and \"nltk_tokenization\".\\n                            - space_tokenization: tokenize_space function is used to tokenize.\\n                            - nltk_tokenization: tokenize_nltk function is used to tokenize.\\n                            - space_punk_tokenization: tokenize_space_punk is used to tokenize.\\n                            \\nvectorizer_type: two possible values \"count\" and \"tf_idf\".\\n                            - count: use transform_count_bow to vectorize the text\\n                            - tf_idf: use transform_tf_idf_bow to vectorize the text\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BHMKa_yFt1W"
      },
      "source": [
        "## 7.2 - Question 8 (1 point)\n",
        "\n",
        "Implémentez la fonction rank qui retourne la liste des indices des rapports de bogues triée en fonction de la similarité entre les rapports de bug (candidat) et du nouveau rapport (requête). Vous utiliserez la fonction de similarité suivante pour comparer deux rapports :\n",
        "\n",
        "$$\n",
        "\\mathrm{SIM}(q,r) = w_1 * F_1(q,r) + w_2 * F_c(q,r) + w_3 * cos\\_sim(\\mathrm{txt}_q, \\mathrm{txt}_c),\n",
        "$$\n",
        "$$\n",
        " F_p(q,r) = \\begin{cases}\n",
        "    1 ,& \\text{si } p_q= p_r\\\\\n",
        "    0,              & \\text{autrement},\n",
        "\\end{cases}\n",
        "$$\n",
        "$$\n",
        " F_p(q,r) = \\begin{cases}\n",
        "    1 ,& \\text{si } c_q = c_r\\\\\n",
        "    0,              & \\text{autrement},\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Où $c_q$ et $c_r$ sont les composants de la requête et du candidat,\n",
        " $p_q$ et $p_r$ sont les produits de la requête et du candidat,\n",
        " $\\mathrm{txt}_q$ et $\\mathrm{txt}_c$ sont les représentations du texte de la requête et du candida. Les paramètres \n",
        " w_1, w_2 et w_3 doivent être réglés.\n",
        " \n",
        "\n",
        "**Pour cette question, la requête doit être retirée de la liste triée (sortie de la fonction rank).**\n",
        "\n",
        "*Pour de meilleures performances, vous pouvez utiliser la fonction cos d'une libraire (par exemple [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)) pour calculer la similarité. Il est préférable de faire des opérations matricielles.*\n",
        "\n",
        "---\n",
        "\n",
        "Implement the function rank that returns a list of reports indexes sorted by similarity of the bug reports (candidates) and new bug report (query). We use the following similarity function to compare two bug reports:\n",
        "\n",
        "$$\n",
        "\\mathrm{SIM}(q,r) = w_1 * F_1(q,r) + w_2 * F_c(q,r) + w_3 * cos\\_sim(\\mathrm{txt}_q, \\mathrm{txt}_c),\n",
        "$$\n",
        "$$\n",
        " F_p(q,r) = \\begin{cases}\n",
        "    1 ,& \\text{if } p_q= p_r\\\\\n",
        "    0,              & \\text{otherwise},\n",
        "\\end{cases}\n",
        "$$\n",
        "$$\n",
        " F_p(q,r) = \\begin{cases}\n",
        "    1 ,& \\text{if } c_q = c_r\\\\\n",
        "    0,              & \\text{otherwise},\n",
        "\\end{cases}\n",
        "$$\n",
        "Where $c_q$ and $c_r$ are the query and candidate components,\n",
        " $p_q$ and $p_r$ are the query and candidate products,\n",
        " $\\mathrm{txt}_q$ and $\\mathrm{txt}_c$ are the query and candidate textual representations, respectively. The parameters \n",
        " w_1, w_2 and w_3 are to be tuned.\n",
        " \n",
        "\n",
        "**In this question, the query has to  be removed in the sorted list (rank output).**\n",
        "\n",
        "*For better performance, you can use the cosine similarity from a library (e.g. [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)). Also, we recommend performing matrix operations.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f37FYL8Ft1W",
        "outputId": "d25b39f1-21fd-48d2-d90c-9e9b047e9a85"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def rank(query_idx, p, c, M, w1, w2, w3):\n",
        "    \"\"\"\n",
        "    Return a list of reports indexes sorted by similarity of the bug reports (candidates) and new bug report (query)\n",
        "    Cosine similarity is used to compare bug reports. \n",
        "    \n",
        "    query_idx: query indexes\n",
        "    p: product values of all bug reports (list)\n",
        "    c: component values of all bug reports  (list)\n",
        "    X: textual data representation of all bug reports  (Matrix)\n",
        "    \n",
        "    w1: parameter that controls the impact of the product\n",
        "    w2: parameter  that controls the impact of the component\n",
        "    w3: parameter  that controls the impact of textual similarity\n",
        "    \n",
        "    return: ranked list of indexes. \n",
        "    \"\"\"\n",
        "    sim = []\n",
        "    COS = cosine_similarity(M, M[query_idx))\n",
        "    for i in range(len(p)):\n",
        "        Fp = 0\n",
        "        Fc = 0\n",
        "        if i != query_idx:\n",
        "            if p[i] == p[query_idx]:\n",
        "                Fp = 1\n",
        "            if c[i] == c[query_idx]:\n",
        "                Fc = 1\n",
        "            sim.append((i, w1*Fp + w2*Fc + w3*COS[query_idx][i]))\n",
        "    sim.sort(key=lambda x:x[1], reverse = True)\n",
        "    rank = [i[0] for i in sim]\n",
        "\n",
        "    sim2 = np.where(p != query_idx) #\n",
        "    sim3 = np.where(sim2 == query_idx,)\n",
        "    return rank\n",
        "\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for i in range(len(x)):\n",
        "        if x[i] == missing:\n",
        "            result.append(value)\n",
        "        else:\n",
        "            result.append(x[i])\n",
        "    return np.array(result)\n",
        "\n",
        "    np.where(x == missing, value, x)\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    You can remove the loop by using the operations == and + of numpy.\n",
        "    \"\"\"\n",
        "    \n",
        "    # pass\n",
        "\n",
        "\"\"\"\n",
        "Hi! Quick question for section 7.2 - We don't understand the use of query_idx. \n",
        "Does it correspond to the index of the bug report for which we want to compute the rank? \n",
        "Or should the rank function calculate the rank of all combinations of the matrix directly?\n",
        "\n",
        "Hello. It corresponds to the index of the bug report for which you want to compute the rank.\n",
        "You'll use it to access the vector representation and calculate the similarity of it and all the others reports.\n",
        "\n",
        "\n",
        "In the question 8, query_idx is the row that contains the vector representation of the query in M. \n",
        "Besides that, the component and product of the query are p[query_idx] and c[query_idx]. (edited) \n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nHi! Quick question for section 7.2 - We don't understand the use of query_idx. \\nDoes it correspond to the index of the bug report for which we want to compute the rank? \\nOr should the rank function calculate the rank of all combinations of the matrix directly?\\n\\nHello. It corresponds to the index of the bug report for which you want to compute the rank.\\nYou'll use it to access the vector representation and calculate the similarity of it and all the others reports.\\n\\n\\nIn the question 8, query_idx is the row that contains the vector representation of the query in M. \\nBesides that, the component and product of the query are p[query_idx] and c[query_idx]. (edited) \\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lSM6p8AFt1Y"
      },
      "source": [
        "## 7.3 - Évaluation / Evaluation\n",
        "\n",
        "Vous allez tester différentes configurations du système de recommandations. Ces configurations seront comparées avec la [mean average precision (MAP) metric](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision). Plus les discussions pertinentes sont recommandées rapidement (c.-à-d. en haut de la liste), plus élevé sera le score MAP. Ressources supplémentaires pour comprendre MAP: [recall and precision over ranks](https://youtu.be/H7oAofuZjjE) et [MAP](https://youtu.be/pM6DJ0ZZee0).\n",
        "\n",
        "\n",
        "La fonction *eval* évalue une configuration spécifique du système\n",
        "\n",
        "---\n",
        "\n",
        "We will test different configurations of our recommender system. These configurations are compared using the [mean average precision (MAP) metric](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision). Basically, the closer relevant threads are from ranked list begining, the higher MAP is. Additional materials to undertand MAP: [recall and precision over ranks](https://youtu.be/H7oAofuZjjE) and [MAP](https://youtu.be/pM6DJ0ZZee0).\n",
        "\n",
        "\n",
        "The function *eval* evaluates a specific configurantion of our system\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyz5u-yCFt1Y"
      },
      "source": [
        "from statistics import mean \n",
        "\n",
        "\n",
        "def calculate_map(x):\n",
        "    res = 0.0\n",
        "    n = 0.0\n",
        "    \n",
        "    for query_id, corrects, candidate_ids in x:\n",
        "        precisions = []\n",
        "        for k, candidate_id in enumerate(candidate_ids):\n",
        "            \n",
        "            if candidate_id in corrects:\n",
        "                prec_at_k = (len(precisions) + 1)/(k+1)\n",
        "                precisions.append(prec_at_k)\n",
        "                \n",
        "            if len(precisions) == len(corrects):\n",
        "                break\n",
        "                            \n",
        "        res += mean(precisions)\n",
        "        n += 1\n",
        "    \n",
        "    return res/n\n",
        "            \n",
        "\n",
        "def eval(tokenization_type, vectorizer, enable_stop_words, enable_stemming, w1, w2, w3):\n",
        "    reports = [r for r in report_index.values()]\n",
        "    report_ids = [r[\"report_id\"] for r in report_index.values()]\n",
        "    prod_v, comp_v, M = nlp_pipeline(reports, tokenization_type, vectorizer, enable_stop_words, enable_stemming)\n",
        "    report2idx = dict([(r['report_id'], idx) for idx,r in enumerate(reports)])\n",
        "    rank_lists = []\n",
        "    for query_id, corrects in test:\n",
        "        query_idx =  report_ids.index(query_id)\n",
        "        candidate_idxs = rank(query_idx, prod_v, comp_v, M, w1, w2, w3)\n",
        "        candidate_ids = [ report_ids[idx] for idx in candidate_idxs]\n",
        "                \n",
        "        rank_lists.append((query_id, corrects, candidate_ids))\n",
        "        \n",
        "        \n",
        "    return calculate_map(rank_lists)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMaHKg_WFt1a"
      },
      "source": [
        "## 7.4 - Question 9 (4 points)\n",
        "\n",
        "Évaluez votre système avec chacune des configurations suivantes :\n",
        "\n",
        "1. count(BoW) + space_tokenization\n",
        "2. count(BoW) + nltk_tokenization\n",
        "2. count(BoW) + space_punk_tokenization\n",
        "3. count(BoW) + space_punk_tokenization + Stop words removal\n",
        "4. count(BoW) + space_punk_tokenization + Stop words removal + Stemming\n",
        "5. tf_idf + space_punk_tokenization\n",
        "6. tf_idf + space_punk_tokenization + Stop words removal\n",
        "7. tf_idf + space_punk_tokenization + Stop words removal + Stemming \n",
        "\n",
        "**Pour chaque configuration :** \n",
        "\n",
        "1. Rapportez la performance avec $w_1=0$ et $w_2=0$\n",
        "\n",
        "2. Réglez les paramètres $w_1$, $w_2$ et $w_3$ et rapportez les valeurs qui donne les 3 meilleurs et les 3 pires résultats.\n",
        "\n",
        "**En plus, décrivez et comparez vos résultats. Répondez aux questions suivantes :**\n",
        "\n",
        "- Quelle méthode de tokenization donne les meilleurs résultats ? À votre avis, pourquoi ?\n",
        "- Les étapes de prétraitement ont-elles un impact positif ou négatif sur notre système ?\n",
        "- Est-ce que TF-IDF permet d'obtenir de meilleures performances que CountBoW? Si oui, à votre avis, pourquoi ?\n",
        "- Est-ce que la comparaison du composant et du produit affecte positivement notre méthode ? \n",
        "\n",
        "**Notez qu'il y a une valeur minimum de  MAP à atteindre pour chaque configuration en dessous de laquelle la question sera pénalisée de 50%.**\n",
        "\n",
        "1. count(BoW) + space_tokenization: 0.090 \n",
        "2. count(BoW) + nltk_tokenization: 0.090\n",
        "2. count(BoW) + space_punk_tokenization: 0.120\n",
        "3. count(BoW) + space_punk_tokenization + Stop words removal: 0.170\n",
        "4. count(BoW) + space_punk_tokenization + Stop words removal + Stemming: 0.195\n",
        "5. tf_idf + space_punk_tokenization: 0.210\n",
        "6. tf_idf + space_punk_tokenization + Stop words removal: 0.210\n",
        "7. tf_idf + space_punk_tokenization + Stop words removal + Stemming: 0.215\n",
        "\n",
        "---\n",
        "\n",
        "Evaluate the system using each one of the following configurations:\n",
        "\n",
        "1. count(BoW) + space_tokenization\n",
        "2. count(BoW) + nltk_tokenization\n",
        "2. count(BoW) + space_punk_tokenization\n",
        "3. count(BoW) + space_punk_tokenization + Stop words removal\n",
        "4. count(BoW) + space_punk_tokenization + Stop words removal + Stemming\n",
        "5. tf_idf + space_punk_tokenization\n",
        "6. tf_idf + space_punk_tokenization + Stop words removal\n",
        "7. tf_idf + space_punk_tokenization + Stop words removal + Stemming \n",
        "\n",
        "**For each configuration:** \n",
        "\n",
        "1. Report the method performance achieved when $w_1=0$ and $w_2=0$\n",
        "\n",
        "2. Tune the parameters $w_1$, $w_2$ and $w_3$ and report the parameter values that achieve the 3 best and 3 worst results.\n",
        "\n",
        "**Also, describe and compare the results found by you and answer the following questions:**\n",
        "\n",
        "- Which tokenization strategy has achieved the best result? Why do you think this has occurred?\n",
        "- Was our system negatively or positively impacted by data preprocessing steps?\n",
        "- TF-IDF has achieved a better performance than CountBoW? If yes, why do you think that this has occurred?\n",
        "- Did the component and product comparison positively affect our method? \n",
        "\n",
        "**Note that there is a minimum MAP value to achieve for each configuration (see below) below which the question will be penalized by 50%.**\n",
        "\n",
        "1. count(BoW) + space_tokenization: 0.090 \n",
        "2. count(BoW) + nltk_tokenization: 0.090\n",
        "2. count(BoW) + space_punk_tokenization: 0.120\n",
        "3. count(BoW) + space_punk_tokenization + Stop words removal: 0.170\n",
        "4. count(BoW) + space_punk_tokenization + Stop words removal + Stemming: 0.195\n",
        "5. tf_idf + space_punk_tokenization: 0.210\n",
        "6. tf_idf + space_punk_tokenization + Stop words removal: 0.210\n",
        "7. tf_idf + space_punk_tokenization + Stop words removal + Stemming: 0.215"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHrGa_Azi7Ux"
      },
      "source": [
        "#X around 1 min\n",
        "\"\"\"\n",
        "You could use grid search or random search (https://scikit-learn.org/stable/modules/grid_search.html).\n",
        "In case of the grid search, you don't need to exhaustively test all possibilities.\n",
        "You could search a space which values are getting the best results. (edited) \n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}