{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fHWJhwOXGzh"
      },
      "source": [
        "# INF8111 - Fouille de donn√©es / Datamining\n",
        "## Automne 2020 - TP3 - Fouille de r√©seaux sociaux / Datamining social networks\n",
        "### Membres de l'√©quipe / Team members\n",
        "    - Pham, Son-Thang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmGvqtSVgfXi"
      },
      "source": [
        "## Instructions de remise / Submission\n",
        "Le travail sera r√©alis√© avec la  m√™me √©quipe que pour les TPs pr√©c√©dents. \n",
        "Vous devez remettre dans la bo√Æte de remise sur moodle:\n",
        "\n",
        "1. ce fichier nomm√© TP3\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb\n",
        "\n",
        "**N.B**: Assurez-vous que tous les r√©sultats soient lisibles lorsque le notebook est ouvert.\n",
        "\n",
        "Tout devra √™tre remis avant le **5 d√©cembre 2020 √† 23h55**. Tout travail en retard sera p√©nalis√© d‚Äôune valeur de 10\\% par jour ouvrable de retard.\n",
        "\n",
        "## Bar√®me\n",
        "\n",
        "Partie 1: 10 points\n",
        "\n",
        "Partie 2: 4 points\n",
        "\n",
        "Partie 3: 6 points\n",
        "\n",
        "Pour un total de 20 points.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Submission\n",
        "The work will be carried out with the same team as for the previous TPs.\n",
        "You must put back in the submission box on moodle:\n",
        "\n",
        "1. this file renamed TP3\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb\n",
        "\n",
        "**N.B**: Make sure that all results are there when you open your notebook.\n",
        "\n",
        "Everything must be submitted before **December 5th 2020 √† 23h55**. Any late work will be penalized with a value of 10% per open day of delay.\n",
        "\n",
        "## Bar√®me\n",
        "Part 1: 10 points\n",
        "\n",
        "Part 2: 8 points\n",
        "\n",
        "Part 3: 6 points\n",
        "\n",
        "For a total of 20 points on 20 points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1et8f3nXGzk"
      },
      "source": [
        "## R√©seaux sociaux / Social networks\n",
        "Les r√©seaux sociaux occupent une grande partie de la vie humaine. Chaque personne appartient tout le long de sa vie √† diff√©rentes communaut√©s. Avec le rassemblage de ces informations sur les diff√©rentes plateformes en ligne de r√©seaux sociaux, les analystes de donn√©es ont voulu exploiter ces donn√©es. C'est un domaine relativement nouveau qui est en pleine croissance avec plusieurs impacts sur plusieurs aspects tels que la publicit√© et les syst√®mes de recommandation. \n",
        "\n",
        "### But\n",
        "Le but de ce TP est de vous donner un aper√ßu de l'analyse d'un r√©seau social.\n",
        "\n",
        "Dans la premi√®re partie, vous impl√©menterez un algorithme de d√©tection de communaut√©s dans un r√©seau social nomm√© LPAm+. Cet algorithme a √©t√© propos√© par [X. Liu et T. Murata en 2010](https://www.sciencedirect.com/science/article/pii/S0378437109010152).\n",
        "\n",
        "Dans la deuxi√®me partie, vous trouverez les personnes avec le plus d'influence dans leur r√©seau social. \n",
        "\n",
        "Pour les deux parties, nous vous fournissons les CSV contenant les r√©seaux sociaux √† analyser.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Social networks\n",
        "Social networks are a major component of the humain life. Each person belongs throughout their life to different communities. With the aggregation of information on various online social media platforms, data analysts were interested in exploiting its data. It is a relatively new field that is growing with impacts on several aspects such as advertising and recommendation systems.\n",
        "\n",
        "\n",
        "### Goal\n",
        "The purpose of this lab is to give you an overview of social network analysis.\n",
        "\n",
        "In the first part, you will implement an algorithm for detecting communities in a social network called LPAm+. This algorithm was proposed by [X. Liu and T. Murata in 2010](https://www.sciencedirect.com/science/article/pii/S0378437109010152).\n",
        "\n",
        "In the second part, you will find the people with the most influence in their social network.\n",
        "\n",
        "For both parties, we provide you with all the csv containing the social networks to be analyzed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "517JwzlPXGzp"
      },
      "source": [
        "# 1. LPAm+ (10 points)\n",
        "\n",
        "## D√©tection de communaut√©\n",
        "La d√©tection de communaut√© dans un r√©seau social est une manipulation fr√©quente lors de l'analyse d'un r√©seau. Une m√©thode de clustering est utilis√©e pour rassembler les personnes dans des communaut√©s selon les liens entre eux. \n",
        "\n",
        "## LPAm+\n",
        "Dans cette partie, vous impl√©menterez l'algorithme LPAm+ pour d√©tecter les communaut√©s parmi les personnages de Games of Thrones. Vous devez utiliser les CSV *nodes* et *edges* pour cela. \n",
        "\n",
        "Cet algorithme consiste √† propager les √©tiquettes dans le r√©seau selon une r√®gle d'√©valuation optimisant la modularit√© du r√©seau. Lorsque l'algorithme atteint un optimum local, il regarde s'il peut combiner deux communaut√©s pour augmenter la modularit√© du r√©seau. L'algorithme choisit toujours la combinaison la plus avantageuse. Si une combinaison est trouv√©e, la propagation des √©tiquettes est refaite. L'algorithme continue tant qu'elle peut am√©liorer la modularit√©. Vous pouvez lire l'article mentionn√© plus haut pour plus de d√©tails, mais cela n'est pas n√©cessaire puisque vous allez √™tre guid√© tout le long du TP. \n",
        "\n",
        "Pour faciliter la repr√©sentation du r√©seau, nous vous proposons d'utiliser le package networkx. La documentation est disponible [ici](https://networkx.github.io/documentation/stable/tutorial.html).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 1. LPAm+ (10 points)\n",
        "\n",
        "\n",
        "## Community detection\n",
        "Community detection in a social network is a frequent manipulation when analyzing a network. A clustering method is used to bring people together in communities according to the links between them.\n",
        "\n",
        "\n",
        "## LPAm+\n",
        "In this part, you will implement the LPAm+ algorithm to detect the communities among the characters of Games of Thrones. You must use the nodes and edges csv for this.\n",
        "\n",
        "This algorithm consists in propagating the labels in the network according to an evaluation rule optimizing the modularity of the network. When the algorithm reaches a local optimum, it checks whether it can combine two communities to increase the modularity of the network. The algorithm always chooses the most advantageous combination. If a combination is found, the propagation of the labels is redone. The algorithm continues until it is no longer able to increase modularity. You can read the article mentioned above for more details but you don't need to as you will be guided throughout the TP.\n",
        "\n",
        "\n",
        "To help you represent a network, we suggest that you use the networkx package.You can read more about the package [here](https://networkx.github.io/documentation/stable/tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DGyw323Srh0",
        "outputId": "12a5000e-2b17-4f11-d86c-1e37afae65d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\sonth\\anaconda3\\lib\\site-packages (1.18.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\sonth\\anaconda3\\lib\\site-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\sonth\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in c:\\users\\sonth\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\sonth\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\sonth\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\sonth\\anaconda3\\lib\\site-packages (3.1.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\sonth\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.6)\n",
            "Requirement already satisfied: numpy>=1.11 in c:\\users\\sonth\\anaconda3\\lib\\site-packages (from matplotlib) (1.18.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sonth\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\sonth\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\sonth\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\sonth\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n",
            "Requirement already satisfied: six in c:\\users\\sonth\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\sonth\\anaconda3\\lib\\site-packages (2.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\sonth\\anaconda3\\lib\\site-packages (from networkx) (4.4.1)\n"
          ]
        }
      ],
      "source": [
        "# vous pouvez bien s√ªr utiliser anaconda pour installer les packages\n",
        "!pip install --user numpy\n",
        "!pip install --user pandas\n",
        "!pip install --user matplotlib\n",
        "!pip install --user networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK6OhgIoSrh5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import random\n",
        "import itertools\n",
        "import math\n",
        "import copy\n",
        "from itertools import combinations\n",
        "\n",
        "class LPAmPlus:\n",
        "    \"\"\"\n",
        "    Contructor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, graph):\n",
        "        \"\"\"\n",
        "        graph gives the graph on which the algorithm will be applied;\n",
        "        \"\"\"\n",
        "        self.graph = graph\n",
        "        \n",
        "        \"\"\"\n",
        "        Ajouter les √©tiquettes initiales aux sommets du graphe dans la fonction __init__.\n",
        "        Il faut que chaque sommet soit dans sa propre communaut√© au d√©but de l'algorithme.\n",
        "        Initialiser le param√®tre labels pour qu'il contient la liste des √©tiquettes pr√©sentes dans le r√©seau.\n",
        "        \"\"\"\n",
        "        \n",
        "        \"\"\"\n",
        "        labels gives all the communities present in the network\n",
        "        \"\"\"\n",
        "        #self.labels = None\n",
        "        self.labels = []\n",
        "\n",
        "        \"\"\"\n",
        "        Assign a label to each node\n",
        "        \"\"\"\n",
        "        #TODO\n",
        "        for i, node in enumerate(list(self.graph.nodes)):\n",
        "            self.graph.nodes[node]['label'] = i\n",
        "            self.labels.append(i)\n",
        "\n",
        "    \"\"\"\n",
        "    Term to optimize when replacing labels\n",
        "    \"\"\"\n",
        "\n",
        "    def label_evaluation(self, current_node, new_label):\n",
        "        #TODO\n",
        "        \"\"\"\n",
        "        Impl√©menter la fonction label_evaluation. Cette fonction retourne la valeur du terme √† optimiser. \n",
        "        Vous pouvez utiliser la fonction linalg.modularity_matrix de networkx pour calculer la matrice B. \n",
        "        Il est normal qu'il y ait une ressemblance avec le calcul de la modularit√© selon la d√©finition que vous avez prise.        \n",
        "        new_label correspond donc √† un  ùëô  possible dans le terme.\n",
        "        \"\"\"\n",
        "        \n",
        "        nodes = self.graph.nodes\n",
        "        index_current_node = list(nodes).index(current_node)\n",
        "        B = nx.modularity_matrix(self.graph)\n",
        "        \n",
        "        lmax = 0\n",
        "        for u, node_u in enumerate(nodes):\n",
        "            if node_u != current_node:\n",
        "                lmax += B[u, index_current_node] * int(nodes[node_u]['label'] == new_label)\n",
        "        return lmax\n",
        "\n",
        "    \"\"\"\n",
        "    Function to choose the new label for a node\n",
        "    \"\"\"\n",
        "    def update_label(self, current_node):\n",
        "        #TODO\n",
        "        \"\"\"\n",
        "        Impl√©menter la fonction update_label. Cette fonction choisit la nouvelle √©tiquette pour le sommet actuel.\n",
        "        En cas d'√©galit√©, la fonction choisit une √©tiquette au hasard parmi les meilleurs.\n",
        "        N'oubliez pas d'enlever les √©tiquettes d√©su√®tes du param√®tre labels. \n",
        "        N.B: Il est possible que la meilleure √©tiquette soit celle actuelle du sommet.\n",
        "        \"\"\"\n",
        "\n",
        "        dict_node_labels = {}\n",
        "        for label in self.labels:\n",
        "            dict_node_labels.setdefault(label, self.label_evaluation(current_node, label))\n",
        "        self.graph.nodes[current_node]['label'] = max(dict_node_labels, key = dict_node_labels.get)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Function that calculates the current modularity of the network\n",
        "    \"\"\"\n",
        "    def modularity(self):\n",
        "        #TODO\n",
        "        \"\"\"\n",
        "        Implement the function modularity in the class LPAmPlus. This function returns the modularity of the network. \n",
        "        You can use the function linalg.modularity_matrix from networkx to calculate B. \n",
        "        You can implement whichever definition for the modularity. N.B: \n",
        "        You can add data to nodes with Networkx to store information about the node. The nodes act like a dictionnary.\n",
        "        \"\"\"\n",
        "        nodes = self.graph.nodes\n",
        "        B = nx.modularity_matrix(self.graph)\n",
        "        m = self.graph.number_of_edges()\n",
        "        n = self.graph.number_of_nodes()\n",
        "        Q = 0\n",
        "\n",
        "        for u, node_u in enumerate(nodes):\n",
        "            for v, node_v in enumerate(nodes):\n",
        "                if nodes[node_u]['label'] == nodes[node_v]['label']:\n",
        "                    Q += B[u,v]\n",
        "                    \n",
        "        return Q / (2*m)\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    Function that applies the LPAm algorithm on the network\n",
        "    \"\"\"\n",
        "\n",
        "    def LPAm(self):\n",
        "        #TODO\n",
        "        \"\"\"\n",
        "        Impl√©menter l'algorithme LPAm dans la fonction LPAm. \n",
        "        Assurez-vous de toujours augmenter la modularit√© lors de vos changements d'√©tiquettes.\n",
        "        N'oubliez pas de garder le param√®tre labels √† jour √† fur et √† mesure lors de vos changements \n",
        "        pour ne pas √©valuer plusieurs fois la m√™me √©tiquette\n",
        "        \"\"\"\n",
        "            \n",
        "        i = 0\n",
        "        n_nodes = self.graph.number_of_nodes()\n",
        "        while(i < n_nodes):\n",
        "            for n in self.graph.nodes:\n",
        "                modularity = self.modularity()\n",
        "                self.update_label(n)\n",
        "                self.labels = list(dict.fromkeys(list(nx.get_node_attributes(self.graph, 'label').values())))\n",
        "                if self.modularity() == modularity:\n",
        "                    i += 1\n",
        "                else:\n",
        "                    i = 0\n",
        "                    continue\n",
        "   \n",
        "    \"\"\"\n",
        "    Function that find which communities to combine and combine them\n",
        "    \"\"\"\n",
        "    def merge_communities(self): # stpham\n",
        "        \"\"\"\n",
        "        Impl√©mentez la fonction merge_communities. \n",
        "        Cette fonction regarde si combiner des communaut√©s augmente la modularit√© et combine le meilleur choix. \n",
        "        Elle retourne True si une combinaison a √©t√© faite sinon False (aucune combinaison augmente la modularit√©).\n",
        "        \"\"\"\n",
        "        modularity = self.modularity()\n",
        "        label_combinations = list(combinations(self.labels, 2))\n",
        "        graph = copy.deepcopy(self.graph)\n",
        "        labels = copy.deepcopy(self.labels)\n",
        "        dict_combinations = {}\n",
        "        for c in label_combinations:\n",
        "            self.labels.remove(c[1])\n",
        "            nodes = [node for node in self.graph.nodes if self.graph.nodes[node]['label'] == c[1]]\n",
        "            for node in nodes:\n",
        "                self.graph.nodes[node]['label'] = c[0]\n",
        "            dict_combinations.setdefault(c, self.modularity())\n",
        "            self.graph = copy.deepcopy(graph)\n",
        "            self.labels = copy.deepcopy(labels)\n",
        "            \n",
        "        best_modularity = max(dict_combinations,key = dict_combinations.get)\n",
        "        \n",
        "        # Elle retourne True si une combinaison a √©t√© faite\n",
        "        if dict_combinations[best_modularity] > modularity:\n",
        "            self.labels.remove(best_modularity[1])\n",
        "            nodes = [node for node in self.graph.nodes if self.graph.nodes[node]['label'] == best_modularity[1]]\n",
        "            for node in nodes:\n",
        "                self.graph.nodes[node]['label'] = best_modularity[0]\n",
        "            return True\n",
        "        # sinon False (aucune combinaison augmente la modularit√©)\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    Function that applies the LPAm+ algorithm on the network\n",
        "    \"\"\"\n",
        "\n",
        "    def find_communities(self):\n",
        "        #TODO\n",
        "        \"\"\"\n",
        "        Impl√©menter find_communities. \n",
        "        Cette fonction applique l'algorithme LPAm+ sur le r√©seau en utilisant les fonctions LPAm et merge_communities.\n",
        "        \"\"\"\n",
        "        self.LPAm()\n",
        "        while self.merge_communities():\n",
        "            self.LPAm()\n",
        "\n",
        "            \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv83c1sWXGzq"
      },
      "source": [
        "### 1.1 Dataset (0.5 point)\n",
        "\n",
        "Nous vous avons fourni les CSV pour toutes les saisons de Games of Thrones. Vous devez maintenant repr√©senter ces r√©seaux en utilisant les deux CSV fournis pour chaque saison: un pour les sommets et un pour les ar√™tes. \n",
        "\n",
        "\n",
        "#### Impl√©mentation\n",
        "1. Impl√©mentez  la fonction  *`load_unweighted_network`*. Cette fonction retourne le r√©seau non dirig√© et sans poids.\n",
        "\n",
        "Utilisez la fonction `test_load` pour v√©rifier votre impl√©mentation de la fonction. Ce test utilise un petit toy dataset. Vous devriez avoir quelque chose de similaire:\n",
        "![title](data/picture.png)\n",
        "\n",
        "\n",
        "---\n",
        "We have provided you with the csv for all the seasons of Games of Thrones. You must now represent each of those networks in code using two csv for each season: the one for the nodes and the one for the edges.\n",
        "\n",
        "\n",
        "#### Implementation\n",
        "1. Implement the function *`load_unweighted_network`*. This function returns a undirected and unweighted graph.\n",
        "\n",
        "Use the function `test_load` to verify your implementation of the function. This test use a toy dataset. You should obtain a result similar to this:\n",
        "![title](data/picture.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUan-Vpy12Ke",
        "outputId": "464e10ef-19b4-452a-e5ab-3157b8c3c93c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Name</th>\n",
              "      <th>Community</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>node_0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>node_1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>node_2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>node_3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>node_4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id    Name  Community\n",
              "0   0  node_0          0\n",
              "1   1  node_1          0\n",
              "2   2  node_2          0\n",
              "3   3  node_3          0\n",
              "4   4  node_4          0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Source  Target\n",
              "0       1       0\n",
              "1       2       0\n",
              "2       3       0\n",
              "3       7       0\n",
              "4       2       1"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "display(pd.read_csv('data/toy-nodes.csv').head())\n",
        "display(pd.read_csv('data/toy-edges.csv').head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wGC7MC3Srh-"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "def load_unweighted_network(node_csv, edge_csv):\n",
        "    #TODO\n",
        "    \n",
        "    nodes = pd.read_csv(node_csv)\n",
        "    edges = pd.read_csv(edge_csv)\n",
        "    \n",
        "    network = nx.Graph()\n",
        "    network.add_nodes_from(nodes.Id)\n",
        "    network.add_edges_from(zip(edges.Source, edges.Target))\n",
        "    \n",
        "    if 'Community' in nodes:\n",
        "      for i, item in enumerate(list(network.nodes)):\n",
        "        network.nodes[item]['ground_truth'] = nodes.Community[i]\n",
        "    \n",
        "    \n",
        "    return network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEwBsNVsjvfs",
        "outputId": "167836ec-5ef9-4674-88ea-d0ae5bba1973"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd1hU19bH8e8MQ1NEBbFiB8RGRFGxgoqKmmLBklhiTUxRY3riqyYa0zQaryXeaBJLTKJBMV6DDY0FlEQQO0XsoCgWBJQ+5/2DMIoMvQzg+jyPj8KcObNH5cdmn3X2UimKghBCiLKhNvQAhBDiaSKhK4QQZUhCVwghypCErhBClCEJXSGEKEOavB6sVauW0qRJkzIaihBCVA7BwcG3FUWx0fdYnqHbpEkTgoKCSmdUQghRSalUqiu5PSbLC0IIUYYkdIUQogxJ6AohRBnKc01XCCFKwu3EFLyDowiLiSc+OR1LMw2OdS0Z3sEWawtTQw+vTEnoCiFKzclrcaw4EMnBiFgAUtK1usfMNDEs8YvAvYUNr7vZ8UzDGoYaZpmS0BVClIqfAy+zwDeM5PQM9O2rlfxvAO85d5NDEbeZNdCRMa5NynaQBiChK4QocZmBG0pSmjbfYxUFktIyWOAbClDpg1cupAkhStTJa3Es8A0rUOA+LilNywLfME5FxZXSyMoHCV0hRIlacSCSES62bH+jG+HzPVnk5aR7zNhIxcqX2uP/fi8ufzEI16ZW2Z6bnJ7BygORZT3kMiWhK4QoMbcTUzgYEUtMfArL/4rk96CoHMcEXbnLW5tOcCs+OcdjigJ/hcdyJzGlLIZrEBK6QogS4x2cGbK7z8aw59xN7j1MzfZ4WobCjwGXCbpyj4xcutaoAO/jOcO6spDQFUKUmLCY+GxlYUWRnK4l7EZCCY2o/JHQFUKUmPjk9BI6T1qJnKc8ktAVQpQYS7OSqUK1NDMukfOUR1KnK4QolvT0dAIDA9m1axd7QhNQmrmh0hT91l4zjRrHetVKcITli8x0hRCFdu3aNdasWYOXlxc2NjZMnz4drVbL11OHYGpmjpFahalGjZFahfqxPwOYGKkx1WRGj7Hm0Z+zKIBXe9uyfktlRma6Qoh8JScnc/jwYXbt2sWuXbu4desW/fr144UXXmDFihXUqVMHAK1Wy7cBvzOoZztm9m2he/5QZ1u+9Yvg233n2f+OG7Y1qwCwYWJnALp/tZ+ouCRUKujVwqZSb4IjoSuEyEFRFM6fP68LWX9/f5ycnOjfvz9r166lffv2GBkZZXtOREQEEydOJNWiHqvSq7F0v/6bHLp//Veur2tqpOZ1d7sSfS/ljSwvCCEASEhI4I8//uC1116jefPm9O7dm1OnTjFx4kSuXLmCv78/s2fPpmPHjtkCNyMjg0WLFtG1a1eGDx/O0T9/Y/azrTA3Lly8GCkZpP7zG0b3o0v6rZUrMtMV4imlKAonT55k9+7d7Nq1i6CgIFxdXfH09OTNN9+kVatWqFSqPM9x7tw5Jk6ciJmZGX///TfNmzcHHm1ak9cuY1lUKjDTGDFrYGsyWt7D3d2d1atXM3jw4JJ6q+WKhK4QT5E7d+6wd+9edu3axe7du6lWrRr9+/fn3Xffxd3dnapVqxboPOnp6SxcuJBvvvmGefPmMXXqVNTq7DPbMa5NcLKtwcoDkfwVHouKR9s5QmaVgkLmGu7r7nY42dYA1ya0bNmSYcOGceLECebMmZPjvBWdSsnjW5CLi4si3YCFqLjS09M5duyYbm02LCwMNzc3PD096d+/v25mWhinT59mwoQJ1KxZk9WrV9OkSZN8n3MnMQXv41GE3UggPjkNSzNjHOtVw6u9/s4RMTExeHl5YW1tzYYNG7C0tCz0OA1JpVIFK4riovcxCV0hKpfo6GjdkoGfnx8NGzbE09MTT09PunbtSkIaRWqdk5aWxhdffMGyZcv44osvmDRpUr7LD8WRmprK9OnTOXToENu2bcPBwaHUXquk5RW6srwgRAkxVB+wlJQU/P39dUEbHR1N3759GThwIN9++y3169cHMve5nbb5dJFa54SEhDBhwgTq16/P8ePHadiwYam9nywmJiasWrWK//73v3Tv3p21a9cycODAUn/d0iYzXSGKKe8+YJnrliXdB+zChQu6JYODBw/SqlUr3Wz2yeoCyL91TpZHF7UyW+ekpKQwf/58vv/+exYuXMi4ceNKdXabG39/f0aMGMH06dP54IMPDDKGwpDlBSFKSVHDrLASExM5cOCALmgfPHigC1kPDw+sra3zGWPBWudkMTdWM7p1FX6b/wbNmzfnu+++082YDSUqKoqhQ4fStGlTfvzxxwJf9DMECV0hSkFRw2zWwJb5Bq+iKJw5c0ZXZfD333/TsWNHXdC2bdu2QLO9k9fiGLU6kOEutni1t6VF3Wr87+R13vU+pTuma3Nr5j/fhvo1zDlxLY53vU8SHZeEkpbCG45pvDdpRLmZWSYnJzN16lRCQkLYtm0bTZs2NfSQ9MordCtXLYYQZaQ0+oDdvXuXzZs3M2nSJGxtbXnhhRe4dOkS06dP5/r16+zfv5/3338fJyenAofgigORJKdncDOXTg41qxizakwHvtkbTrv5ezgVHcfyF50BUBubctGsebkJXAAzMzN++uknJk2aRJcuXdi3b5+hh1RociFNiCLICrNxXRrrnUE6N6zB230daNugOhlahcBLd/nkf2eJTUjR9QFb8aIzwcHBuiWDM2fO0KNHDzw9Pfnwww+xs7MrVuBltc5RlMxODgBtG1SnXnUz3TGerety/mYivmcyH//W7zwhs/vS3KYqF2If6FrnlKe9EFQqFdOnT6dt27a8+OKLfPjhh8yYMaNcfXPIi8x0hSikx8MstxlkdXNjfv3nKt2//otuX+/nQUq6rkGjosDu09ep09iOiRMnEh8fz7x587h16xZ//vkn06ZNw97evtghktU6Jy8OdaoReiNe93FSWgZX7jzEvnbm1orluXVOr169CAwMZO3atbz88sskJSUZekgFIjNdIQrp8TDLbQZ54N9Khizrjl5m0ytddB8bGal4b6U3HzzfoVhjycjIICUlRfcrNTVV9+cj527m2zqniomGuw+yN4FMSE7DwjQzGsp765wmTZpw5MgRJk6cSM+ePdm6dWuu5WyGKul7koSuEIVUlD5gnZtacf7mo/BKV9T87/BxLu9cozcwc/vckx8rioKpqanul4mJie7Pqa4TobZjnuN6mJqOxRNdGizMNCSmPGq7U95b51SpUoVff/2VhQsX0rlzZzZt2kSPHj10j+dd0pd3fXJpkNAVopAK2wfMsW41pve2Z8qG7JVAxlUsadu0ba6hWZCPjYyMcl2GeGtTCNtOXM9zbBE3Exj22Ibh5sZGNLaqyvlbj75BVITWOSqVSneRcdiwYcybN49XX32VjX9fybOkL2sviD3nbnIo4naRS/oKQ0JXiEIqTB+wxtZVWDu+E5/uOMexy/eyPdauVQteH9mupIen41jXElNNDCnpWozUKjRqVbZODulahd3nbvLRwJZ4tq7LX+G3mNHHnrCYeC7EPgAqXuscT09PAgICGDx4MNvO3uVyTWeSC1BhoiiZ69kLfEMBSjV45UKaEIWUGWb5f+k0qGHOxkmdWbb/PD4h2feIVdJSOLrLm++++47Lly+Xyji9OjyawU7rZUf4/AG87m7HUGdbwucPYFovO+4+SOW1n4N5r18LTs7pR7uGNZj2a8ijcVLxWufY29uzZutewqu0KlDgPi6vkr6SIjNdIQrJq4MtS/wiAHKdQdayMOGXyZ1Zf/QKG/+5muMcpmZmTOrdhsN7ffnkk0+wsrJiwIABDBgwgJ49e2JqWvwLO7UsTHFzsGFv6E2+3Xeeb/ed13tcwIU79FlyMMfnK3LrnJ/+uQ5Gxoxz1V/S97gZfeyZ6eHA6DWBBFy4oyvpWzVG770NxSahK0QhPR5m03rZ8ZbHo92vsnqBKUBj66rM6GPPjD72usdbf7IblQp6O9ZmypiBTBk7Cq1Wy/Hjx9m5cydz587lzJkzuLm5MXDgQAYMGFCgrRNz84a7HYfP3yYpLaPQzzXTGFXI1jn6Svp62ttgpqeTRSOrKgxoU5eb8cm6zykKpVqfLLcBC1EEWbfXFiXMzI2N2PSKa+am3XpkbTS+c+dOdu3aVexZcGnerlwerTp4gSV+EdmqFN7p60C96mY5Zrprx3dk7dHLfPZCGz7YcoqAC3eAzLXsmX0deLVn4fcbBtnaUYgS90zDGswa6FjEMHPMNXABrK2tGTVqFKNGlcwseIxrE86cPctvYSmoNKbksS8PKFqMUJg1sDWebeqx6uAFg9e1FlZBS/oGtqlLWoaWA+GxOR4rzfpkCV0hiqgwfcBQtJibGBe6JEmtVuPi4oKLiwuzZ8/ONgsu6FrwnTt32DDnFT5buZ6/E6rn2Tqna5Oa7Prha3Y0nM78PzOv5Bu6rrWwClLSV8XEiPf6OzL2x7/zOE/p1CdL6ApRDAXpA6ZVFB5GBrNohheDivnjekFmwVkh3LRpUxRF4bXXXmPkyJFMeKEPE8i7dc7OMzc47D6JwGsPQJVzDdQQda25ycjI4MqVK5w/f56IiAjd72E1O0OTTnk+d6aHAz4hUUTdy/3W4dKqT5Y1XSFKSF5htnLJ10RERLBhw4bSe309a8FNmzbl7NmznDp1iurVq+f5/PK49qsoCtevX88RrBEREVy6dInatWtjb2+Pg4MDDg4OVK9ene3nkziR0QCMHoXmk2u6vtO6U7e6ORnazPdqVdWUhOQ0Vh28wKpDF0t1TVdCV4gyEB8fj729Pfv376d169al/nparZZdu3YxfPhw7OzsuHTpUo5Z8OPy23fXrrYFi4c/Q2PrzI3DT0ff55P/nSXyVmK+FwbzoygKd+7c0RuskZGRVK1aVResjwds8+bNMTc35+zZs2zbtg0fHx+uXr1K/+e9CKwziHRFpSvpm9HHnrrVzfho62nStQrVzDQYP9Zl+I83uvHZn+c4EBHLw9QMTDVqjnzQu8hr1xK6QpQDixYt4ujRo2zZsqXUX0tRFPr370+PHj2YPXs2d+/eZc+ePblWREzbfJq9oTfp16ouiqLoSqyyQtfSTIOlmTFRcUmoVTCuSxNGujRkwH8Oo1JB/1Z18q1rTUhIyBGsWb9nZGTowvTxYLW3t88xQ9dqtQQGBuqCNjU1lcGDBzNkyBC6d++ORqPhlQ1B7A29yYze9tlK+gC+9YvIUbPs/34vXfVCQd9PXiR0hSgHkpKSsLOzY/v27XToULzdxfKzcuVK1q1bR0BAABpN9ks3Wq2WkJAQfH192blzJ2cvXMVq/AoU9aPjciuxgswbQl7q1IiPB7Sk5dxdALqZYVWNwoULF/QG6/3797Gzs9MbrDY2NnluZZmamsr+/fvZtm0bf/zxB7Vq1dIFrbOzc47nlmZJX0FI6ApRTqxcuZIdO3bg6+tbaq8RERFBt27d8Pf3p0WLFvkev3jnaVYevkK68ii4cgvdU3P6UcXECLVKxWK/CJb/FQmASpuGcvJ/xBzYSOPGjbMFa9bvDRo0QK0u+M4DCQkJ7Ny5k23btrFz505atmzJkCFDGDx4MPb29vk+35Br1FKnK0Q5MXnyZL7++msCAgLo1q1biZ8/PT2dcePGMXfu3AIFLsDV+PRsgZsXp3l7MDc2YlgHW6Ifu/KvqI3xGD6eVTv+m2NmXRi3bt1i+/btbNu2jUOHDtG1a1eGDBnCN998Q7169Qp1rsKU9BW3cWhhSOgKUYZMTEyYO3cus2bN4q+//irxFjNfffUV1apV4/XXXy/wcwq7VWVSWgYb/77C8f/ri8fig9x5kAqAojErUuBeunRJtz576tQp+vXrx+jRo9m4cWO+FRf5KUhJn0LmHhOvu9sVa0mhoCR0hShjY8eO5csvv2Tfvn14eHiU2HlDQkJYunQpwcHBhfoxvjBbVWZRq1SYGxtR19JMF7oFrWtVFIVTp07pgvb69es8//zzfPDBB/Tp0wczM7P8T1IITrY1WDXGJc+SPukcIUQlptFo+PTTT5k1axZ9+vQpkdlucnIyY8eOZfHixbm2q8lNQfbd7dLMmnsPUgmNiaeKiYZ3+jlwPymNyNhEIP99dzMyMjh69Cg+Pj5s27YNrVbLkCFDWLZsGV27dsXIyKhY778grC1Mi1x3W5LkQpoQBqDVanF2dmb+/Pk8//zzxT7fu+++y5UrV9i8eXOhQ/x2YgpdvvAjTQtv9dFfYhVxM4F3+ragbnUzktMyOBV9n692hREWk7k/gb661uTkZPbv34+Pjw/bt2+nbt26DBkyhCFDhhSqjXxFJNULQpRD27dvZ/bs2YSEhBRqOeBJBw8e5MUXX+TUqVPUqlWrUM+9fv06H3/8MftSm2PU2JnM/r+F83hd6/3799m5cyc+Pj7s3r2btm3b6ioOmjVrVuhzV1R5ha50jhDCQJ577jnMzMz4/fffi3yO+Ph4xo8fz/fff1+owE1KSmLBggU4OTlRr149Ns2ZiLlx0VYbTY3U1Lt3mgEDBtCwYUM2bNhA3759CQ8P5/Dhw7z99ttPVeDmR9Z0hTAQlUrFZ599xptvvsmwYcOKdOX/7bffxsPDg2effbZAxyuKwu+//87777+Pi4sL//zzjy4QZw1MK3RdqyojlbsHfyassQnjx49n06ZNWFpaFvp9PE0kdIUwIA8PD+rVq8fPP//M+PHjC/Xc7du3s3//fk6ePFmg44OCgnjrrbd48OABa9euxd3dPdvjhalrVbRajFRaRrQw4dP5v5RIe6GnhazpCmFg/v7+jB07lvDwcExMTAr0nNjYWJ555hk2bdpEjx498jz2+vXrzJo1i127dvHZZ58xfvz4PKsFTkXF6epatVotj098jZQM1EZG9Haswxu9yqautSKSO9KEKMe6d++Oo6Mja9asKdBNDYqi8OqrrzJmzJg8AzcpKYklS5awePFiJk+eTHh4eL4/+iclJXEl5DCp+324u/cAtVwG0ah1R6zqNsDWxgrHepZlXtda6SiKkuuvDh06KEKI0hcUFKTUr19fefjwYb7Hrl+/XmnTpo2SlJSk93GtVqts2rRJady4sTJ06FAlMjIyz/Pdu3dP+fnnn5Vhw4YplpaWipubm7JkyRLl0qVLRXkrQlEUIEjJJVdlpitEOdChQwc6d+7MypUrefnVN/EOjtLbm+zhvVu888477NmzR++dW8HBwbz11lskJCToXbfNcv36df744w98fHwIDAzE3d2dwYMH891332FjY1PK7/bpJmu6QpQTPgePM33VDsybu6BSqZ7oTZa5R4DmVjj9GsKS/3sr23Nv3LjBxx9/zK5du5g/fz4TJkzIsW4bERGBj48PPj4+REREMHDgQAYPHoynpycWFhZl8RafGrKmK0Q593PgZRbsu4VRY2dSMxR4omdv1iYtKTWbszNVQ4fAy4xxbUJycjKLFy9m8eLFTJo0Kdu6raIoBAcH64I2Li6OF154gXnz5uHu7l7gi3aiZEnoCmFg2fZ91dMMMhuVmuQ0LQt8Qwk+fpxtC9/B2dmZv//+m+bNm5Oenq679Xbbtm2Ym5szZMgQfvzxRzp16lSsO99EyZDQFcKATl6LY4FvGMNdGurtTWZbwxz/D3rzIOXR9ourDl1g2f5IfC6pmLNkNaP6dWXPnj3MmzePHTt20LRpU4YMGcLu3btp2bJlpd7joCKS0BXCgFYciCQ5PYOb8Sks/ytS15vsSU7z9pChfeL6i8aYpXvDeGvcUFxcXBgyZAifffZZoXcZE2VLQlcIA7mdmMLBiFgUBXafjQGgbYPq1Kte0P1kVTyo3oTjZyOwa1i39AYqSpSErhAG4h0cVeBjA97vjYKC//nbfL4zlHsP0wDQGBmx79ID7GRyW2HIqroQBhIWE5+tLEyfuw9TeW65P92+3s9zy/2paqph6Uhn3ePJ6VrCbiSU9lBFCZLQFcJACtKb7GFqBqej75OhVbidmMrc7Wfp6WCDhemjH1Ljk9NKc5iihEnoCmEgRelNpvxbv/t4PUJBe5OJ8kFCVwgDyexNlvklaPRvP7LHe5MZqVW0a1iDZrWqolJBjSrGfPJca45euEPCvyVk+fUmE+WPXEgTwkC8OtiyxC8CgGm97LL1JhvqbMu3fhFcvP2A9/q1wNrChMTkdA5H3mb6byG64xTAq71tWQ9dFIOErhAGUsvCFDcHG/aei+Hbfef5dt95vcdtP3ld7+dVKujVwka2WaxgZHlBCANRFIUa0X+jTU8t0vPNNEa87m5XwqMSpU1CVwgD0Gq1zJw5E9+fv+Od3s0w13MXWl7MjdXMGugonRsqIFleEKKMJScnM27cOG7evMnhw4epWbMmVjVrFKg3mUqVOcOdNdBR19NMVCwSukKUoXv37jF48GDq1KnD7t27dRuRj3FtgpNtDV1vMhWPtnOER/vp9mphw+vu0pusIpPQFaKMXLt2jQEDBuDh4cHixYtzbLPoZFuDVWNcuJOYgvfxKMJuJBCfnIalmTGO9apJb7JKQkJXiDJw+vRpBg0axIwZM3j77bfz3G7R2sKUV3s2L8PRibIkoStEKTtw4AAjRoxg6dKlvPjii4YejjAwqV4QohRt2rSJESNG8Ntvv0ngCkBmukKUmiVLlrB48WL8/PxwcnIy9HBEOSGhK0QJ02q1vPfee+zcuZOAgAAaNWpk6CGJckRCV4gSlJKSwssvv0x0dDT+/v5YWVkZekiinJE1XSFKSFxcHJ6enqSnp7N3714JXKGXhK4QJSAqKooePXrQpk0bNm3apLvpQYgnSegKUUxnz56la9eujB07lv/85z8YGRkZekiiHJM1XSGK4dChQwwfPpzFixczevRoQw9HVAASukIU0e+//84bb7zBL7/8goeHh6GHIyoICV0himDp0qUsXLiQPXv20K5dO0MPR1QgErpCFIJWq+WDDz5gx44d+Pv706RJE0MPSVQwErpCFFBKSgoTJkzgypUrBAQESEmYKBKpXhCiAO7fv8/AgQNJSkrCz89PAlcUmYSuEPm4fv06PXv2xNHREW9vb8zNzQ09JFGBSegKkYdz587RtWtXXnzxRZYvXy41uKLYZE1XiFwcPnwYLy8vFi5cyLhx4ww9HFFJSOgKoceWLVt47bXX+Pnnn+nXr5+hhyMqEQldIZ6wbNkyvvzyS3bv3o2zs7OhhyMqGQldIf6l1Wr56KOP2LZtG/7+/jRt2tTQQxKVkISuEEBqaioTJ07k4sWLBAQEUKtWLUMPSVRSErriqRcfH8/QoUOxsLDAz8+PKlWqGHpIohKTkjHxVMuqwbW3t2fLli0SuKLUSeiKp1ZoaChdu3ZlxIgRrFy5UmpwRZmQ5QXxVAoICGDo0KF89dVXjB8/3tDDEU8RCV3x1PHx8eGVV15hw4YNeHp6Gno44ikjoSueKitWrGDBggXs2rWLDh06GHo44ikkoSueCoqiMGvWLLy9vfH396dZs2aGHpJ4SknoikovNTWVKVOmEB4ezpEjR6QGVxiUhK6o1BISEhg2bBhmZmbs379fSsKEwUnJmKi0YmJicHNzo2nTpmzdulUCV5QLErqiUgoPD6dr164MGTKEVatWodHID3WifJD/iaLSOXr0KEOGDOHzzz9n4sSJhh6OENlI6IpK5Y8//mDy5MmsX7+eAQMGGHo4QuQgywui0li1ahWvvfYavr6+Erii3JKZrqjwFEVh9uzZbNq0icOHD9O8eXNDD0mIXEnoigotLS2NKVOmEBoaypEjR7CxsTH0kITIk4SuKFduJ6bgHRxFWEw88cnpWJppcKxryfAOtlhbmGY7NjExES8vLzQaDfv376dq1aoGGrUQBSehK8qFk9fiWHEgkoMRsQCkpGt1j5lpYljiF4F7Cxted7PjmYY1iImJYdCgQbRv357vvvtOSsJEhSH/U4XB/Rx4mQW+YSSnZ6AoOR9P/jeA95y7yaGI20x2sWLFW6N4+eWXmTNnDiqVqoxHLETRqRR9/8v/5eLiogQFBZXhcMTTJjNwQ0lK0+Z/8L+UtBQG1kviu3dGl+LIhCg6lUoVrCiKi77HpGRMGMzJa3Es8A0rVOACqIxNOXDfmlNRcaU0MiFKjywvCINZcSCS5PSMbJ87+0n/bB+bGRuxIfAKn/zvbLbPJ6dnsPJAJKvG6J1MCFFuSegKg7idmMLBiNgca7itP9mt+7O5sRFBszzwPX0jx/MVBf4Kj+VOYkqOqgYhyjNZXhAG4R0cle8xA9vW5c6DVP65fFfv4yrA+3j+5xGiPJHQFQYRFhOfrSxMn2HtbdmaR6gmp2sJu5FQ0kMTolRJ6AqDiE9Oz/Px+tXN6NzUOt+ZbHxyWkkOS4hSJ6ErDKJKPlcThra3JejyXaLuJeV5nKWZcQmOSojSJxfSRJmIj4/H39+fgwcPcvDgQS6YNKWq60gw0h+aQ9s34LsDF/I8p5lGjWO9aqUxXCFKjYSuKBX37t3j8OHDupANDw+nY8eOuLm5MXPmTPyDTrJdUdB3L1n7RjWpa2mmt2rhcQrg1d62VMYvRGmR5QVRIm7fvs3WrVuZMWMG7dq1o3HjxixfvhwrKyu+/fZbYmNjmTt3LqdPn+a1115DnfqAbs1qou8OXq/2Ddh1NoYHqRk5H/yXSgW9WthIuZiocGSmK4okJiZGN4s9dOgQ165do1u3bri5ubFq1So6dOiAsbExDx8+ZOPGjbz++uukpqYybdo0fvrpJ6pVq8bJa3GMWh1IUlr2cP1425l8X99MY8Tr7nal9faEKDUSuqJAoqKidCF78OBBYmNj6d69O25ubkycOJF27dpl2+nr8uXLrFy5kp9++glXV1cWLVqEh4cHavWjH66eaViDWQMdC733grmxmlkDHXGyrVGi71GIsiChK/S6fPlytpC9f/8+PXv2xM3Njddff522bdtiZGSU7TmKorB//36WLVvG4cOHGT9+PIGBgXl2chjj2gSAz3xDSUpJR6XOfcVLpcqc4c4a6Kh7nhAVjYSuQFEULly4kC1kk5OTcXNzw83NjbfffptWrVplm6U+LjExkQ0bNrB8+XIApk2bxs8//4yFhU2hVGMAACAASURBVEWBXn+MaxMuBB3g97P3SavlgIpH2zlCZpWCQuYa7uvudjLDFRWahO5TSFEUwsPDs4WsSqXCzc2Nnj178vHHH9OiRYt896m9cOECK1asYN26dfTs2ZNly5bRq1evQu9vm5GRwc9LF7BmzRradHDF+3gUYTcS8N7uy6C+7jg3q4NX+5ydI4SoiCR0nwJarZZz585lu/BlZmaGm5sbHh4ezJ8/n2bNmhUoLLVaLX5+fvznP/8hMDCQiRMnEhwcTJMmTYo8Pm9vb2xsbOjZsycqlYpXe2YuR+z+9CXe/Oh52rSRRpOi8pDQrYS0Wi2H/glhzb4znI2OIzYuAWMlnebWZgzzfI6FCxfSuHHjQp0zISGBdevWsXz5ckxNTZk2bRqbN2+mSpUqxRqroih88cUXzJ8/P0fo16xZk3v37hXr/EKUNxK6lUB6ejonTpzg4MGD7Pz7HKHqRhg3ega1ugba6rXQVM+8kSBKo+bbS3DicCyvq6vzTMP810YjIiJYvnw5P//8M3369OH777+nR48eJdYiZ9euXWRkZDBo0KAcj0noispIQrcCSktLIygoSLdUEBAQQMOGDWnsMZYrdkMwUVQowJNFWE/2GsutCkCr1bJr1y6WLVtGcHAwkydP5uTJkzRs2LDE38sXX3zBhx9+qPcinZWVlYSuqHQkdCuAlJQU/vnnH92abFYZlpubG5MnT2bdunXsvvCABb6hpBWg3lVRICktgwW+ocCjsq379+/z008/sWLFCqpVq8b06dPx8fHBzMysVN5XQEAAUVFRjBw5Uu/jMtMVlZGE7mNuJ6bgHRxFWEw88cnpWJppcKxryfAOZXvlPCkpicDAQF3IHjt2DEdHR9zc3HRrqTVr1tQdX9ReY0lpWhb4hlEtLY49v63ml19+oX///qxdu5auXbuWepfdL774gvfeey/X9uk1a9bk7l39G5gLUVFJ6JIZWisORHIwIhYg2+baZpoYlvhF4N7Chtfd7Aq0DlpYDx484MiRI7qQDQkJoU2bNri5ufHee+/RrVs3qlevnuvz9fUaA7CtYc78wW1o36gmqekZ+J6JYd6Oc2RoH/XISUpN482V25lgb82ZM2do0KBBib8/fU6dOkVwcDDe3t65HmNlZcX58+fLZDxClJWnPnQzW4CHkZyekaNfFxR8HbQw4uPjCQgI0IXs6dOnadeuHW5ubsyePZuuXbsW+MaC3HqNAcwf3IY7iSl0+twPSzNjNkzqxFjXxqw9cvnRQSo15nYdmfF+7zKdzX/55Ze89dZbeS5dyPKCqIye6tDNDNyC3fef2zpoQcTFxWXb5jA0NBQXFxfc3NxYsGABrq6uRS69yqvXWMOaVVh39DIp6Vpi/w1nh9o5w1xNZq+xrPrY0nbx4kX27NnDqlWr8jxOQldURk9t6BZ3HdTJtkaut6Pevn07W8hGRkbSuXNn3Nzc+Oabb+jUqVOJXZzKq9fYTwGXeM6pPoEX71Dd3Bh3h9os3hue47iy7jW2cOFCXn31VSwtLfM8TtZ0RWX01IZubuug1c2N+XqYEz3sa3H3QSpf7w5n+8nr2Y5JTs9g5YFIVo1xAeDmzZvZ7va6evUqXbt2xc3NjRUrVuDi4oKJiUmpvI+8eo0FXrrLqE6NODO3PxojNd7B19h97mYu5ymbXmMxMTFs2rSJsLCwfI+VkjFRGT2VoZvnOugLrUnL0OKywI9W9Sz5cXxHQm/Ec/5Wou4YRQG/czFMmDqNowf2cvPmTd02h+PHj8fZ2TnXK/LFlZyczOnTpwkJCSEkJIQjyY2grlOO41QqWD+xE7/8fZVh3x2hiqkRC4c58aGnI1/uyhl4ZdVrbMmSJYwePZratWvne6wsL4jK6KkM3dzWQc2NjfBsXY/+Sw/xMDWDoCv38Au9yVDnBny1O/uP5RkZGaQ17MCvv07EyckpxzaHJSEuLo6QkBBOnDihC9nIyEgcHBxwdnbG2dkZy+qt2HYhjZT07N9Bapgb06CGOeuPXiY1Q0vqQy2/B0fxTt8WOULXCC11zXLv0lCS72fNmjUEBwcX6Pis0FUUpdTL14QoK09l6Oa2DtqsVlW0isKl2w90nwu9kUDnplY5jlXUGmzs2uHs3K7Y41EUhevXr+uCNetXbGwsTk5OODs707NnT2bMmEGbNm0wNX1UZXA7MYVtX+0n80bfR+49TOPq3YeMcW3M94cvUtXEiGHtbQmNidf7+gvf8GLr57Xx8vJi2LBhNGvWrNjv60krVqxg0KBBBd4cx9TUFI1Gw4MHDwpczSFEefdUhm5u66BVTI1IeGJtMyE5DQtT/X9NRVkH1Wq1REZG5ghYrVarm70OHz6czz//HHt7+3xn0LUsTHFzsGFv6M0cyyVTfw5mzrOtmOrWnAytwtGLd5i/41y2Y1Qq6Ne6PssuRXDw4EG8vb3p0qULDRo0wMvLCy8vLxwcHAr9Pp/08OFD/vOf/7B///5CPS9rXVdCV1QWT2XoWprpf9sPUzKwMM2+tmlhqiExRX9I57cOmpqaytmzZ7OF66lTp7CystIF7JtvvomzszMNGjQo8o/Qb7jbcfj87Ry9xs7diGfU6sA8n5vVa8zY2BgPDw88PDxYsWIF/v7+eHt74+7ujrW1tS6AW7VqVaRx/vDDD3Tp0oXWrVsX6nlZSwylse+DEIbw1IXu9evXuXPhNGRYglH20Lx4+wFGahVNrKtw+c5DAFrWs+T8zZzlVGYaNY71quk+TkhI4OTJk9kCNjw8nKZNm+oCdsiQIbRr1w4rq5zLFcVR1F5jpKfSSrlCm/r9sn3ayMhI1zVi6dKlHD16lC1btjBgwACqVq3KsGHD8PLy4plnnilQAKelpbFo0SI2b95c2LcmF9NEpaNS9F3C/5eLi4sSFBRUhsMpHTExMWzZsoXNmzdz6tQpPAcP51i950lXcgbGslHOKCh8sOU0repb8tP4jgz77ki26gUAjRpGVz1H2MkgQkJCiI6OpnXr1rqAdXZ2pm3btsXeb7Yw8ru7LktWr7EZbo3YMGcKjRs35qeffsq2VqyPoigcO3aMLVu24O3tjUql0q0Bu7i45BrA69atY926dYVaWsjaB2PVb/+jfpNmODRpaJB9MIQoCpVKFawoiovex8o6dMtqU5mbN2/qgvbkyZM8++yzjBgxgn79+mFqasorG4L0roNWNzdm4TAnutvX4t7DNL7aFZajTlfRaql6LxJPi2u6gHV0dCy1MrHCOBUVx8oDkfwVHlugXmNJSUmMGTOGu3fv4uPjQ40aBdtbQlEUTpw4gbe3N97e3qSkpDBs2DCGDRuGq6urbqtGrVZLmzZtWLp0KX379s33vHnvg5E5/tLcB0OIklAuQrcsvphu3brF1q1b2bx5M8ePH88WtE/eAXbyWhyjVgfmWActkPQUOsYHsPqr2QUOqbJ2JzFF12ssPjkNSzNjHOtV09trLCMjg7fffpt9+/axc+fOQq+fKorC2bNndQF879493RLErVu3+PLLLzl27Fi+SxGFnalLV2BRXhk8dEvziyk2NpatW7fy+++/ExQUxMCBAxkxYgT9+/fH3Nxc73MePnzIqVOn+PFQBH53q6NVFXyGam6s5p0+zQj+7Vt27NjB6tWr8fT0LPDzyytFUViyZAlLlizhzz//xMkp5w0XBRUWFqZbgjhz5gy9e/fm/fffx83NLdefBgqzD0YWc2M1swa2lOAV5Y5BQ7c0vphu376Nj48Pmzdv5tixYwwYMIARI0bg6emZI2jv3r2bozzr8uXLODo64uzsjMq+J4cSa5GW8WSla3b6viHs27ePSZMm0adPHxYvXpzn9osVxebNm3nzzTf59ddf6dOnT7HOtX//fiZPnsyrr77Kli1buHTpEoMHD2bYsGH07t1bd2t0bj91NLexYP4LrWnToDp3H6TyhW9ojtuYzY2N2PSKq7RlF+WKwUK3OD/CP/nFdOfOHXx8fPj9998JDAzE09OTESNGMGDAAKpUqYKiKERFReUI2Hv37vHMM89ku8DVqlWrbHsh5LUOaqwGtVqdbR30cQkJCbz33nv4+vqyevVq+vfvX7S/rHLk0KFDDB8+nEWLFjF27Ngin6dv37689NJLTJgwAYDLly+zdetWtmzZQlhYGM899xzDhg3jj7t12Bee/bZsI7WKvTN7svHvq/wUcInOTa354WUXBi3zz3bzikoF/VvV0e2DIUR5YLDQze1i1ZIR7ejW3BpzEyNiE1P478GLbAq69sSgoZedFd0JZfPmzRw9epT+/fvr1mijo6OzheuJEycwMjLKFq7Ozs40b95cb/8tfZ5cBw0/HUIjSw3L3h6d70U+Pz8/Jk+ejIeHB998802Fn/WeO3eOgQMH8sorr/DRRx8VujY3KCiIoUOHEhkZqXezn+joaLZu3cqmP3y55vwKKk32YxzqWODzWjdaf7Jb97n1Eztx4loci/dGZDvWVKPmyAdlux+wEHnJK3RL7XJ7XpvKrDwQyQdbTpGaoaW5TVV+m+LK2ev3OXP90S2qigL7QmO4dWkvbm5uPPvss5w7d45vvvmG8ePHU7t2bV2wzpw5E2dnZ+rVq1ese/StLUyz7Sm7cWME27Ztw9piYr7P9fDw4PTp07z33nu0bdu2ws96W7VqxZEjRxg0aBBXr15l+fLletdjc6tG8V2xhHfeeSfX3dUaNGjAtGnTMHYayOK94aRmZP+PoiLnv6MKaFGnmt7Pl+V+wEIUR6mFbl6baz+5Y5eiQGPrqtlCFzK/mI7e0HL11191ATtixAjatWtXJlUDrq6ufPjhhwU+vlq1aqxatUo36+3bty+LFi2qsLPe+vXrc+jQIby8vBgyZAi//fYbVatWBfKuRjExukFKYy9sqtfn5LW4PKtRwmLicwQuwIXYRO48SOXVns34wf8SXZpb07mpNYEX7+Q4tqz3AxaiOEotdPPaXBtg/gtt8Gpvi7mJEWei7/NX+C09ozNh9Jvv858XO5TWMPPUrFkzkpOTiY6OLlTvMA8PD06dOqWb9a5Zs4Z+/frl/8RyqFq1auzYsYNXXnmFXr16sWPHDvZcfJhnNUpqhoJKY8L+iNscuXgvz2qU3PbBSNcqvLIhiE+fa81Ut+acjrrPn6dvkJrL/6my2g9YiOIqtdDNa3NtgNl/nGHu9jO0b1QT12bWuX4xPUgtXGeHkqRSqejcuTN///03Q4cOLdRzLS0t+e9//8vevXuZPHky/fr145tvvsm3W0J5ZGxszI8//sinn35K5zHvYdJpJCl6ZqdPyq3F0c2bNwkICMDX15dDKY3Btr3e54fFJDDysb0jtkztypbj+n+CKqv9gIUoroJdYSqC3DaVeZxWgaAr96hX3Ywxro31HnPsyCFmzpzJ999/z6FDh4iNjSWvi38lrXPnzgQG5r1pTF769u3L6dOnUalUtG3blj179pTg6MqOSqViyKS30Lh4FShwH5eUpmXOtpO06u5JlSpVqF+/PsOHD8fHx4fUW5dQ0lP0Ps+xbjVMNWrMjNVM6dGM2tVM9S5bPbkPhhDlWanNdB3rWmKqiclziSGLkVpFY6ucexQYq6GXswP1E7QEBgaydu1aQkNDUavVtGzZEkdHR93vjo6ONGnSpMQ3E3d1deWzzz4r1jksLS35/vvv2bNnD1OmTKmws94VByJJz+X79HNO9ZjRx4H6NcyITUjhXe+THLv8aKOaDEWN0rIfz9a35N69exw7doyWLVvS38OJ9ffNSNMT5EOcGzCqYyM0ahXHLt9lzI9/k5qR8/+TAni1ty2x9ylEaSq1krHbiSl0+2p/jtC1rmpC1+bW7Au7RXJaBt3tarFqTAdm/HaCvaHZC9+V9FQaBH3HxNEjGD58ONWrV0dRFG7dukVYWBihoaHZfo+NjcXOzi5HIDs4OBR545m4uDhsbW2Ji4srkb0V4uPjeffdd9m9ezerV68u0FpvWe1Xkd8Y9P17AnS3q8WXQ9sy7dcQTkTFUbta5phuxmefwSrpqbSM2IjXc54899xz1KlTB8i9tLAgpE5XlEflqk7XqqoJ373Unpb1LFGpIDouibVHLvPbsZx1un0da/NsjRusX78ePz8/PD09GTduHP369dMbgImJiUREROhCOCuQL1y4QN26dbMFcdbvNjY2+b6PVq1a8csvv9CuXfG7RGTJmvX279+fRYsW6Z31lofNX+Li4jhx4gTfH77E0UQrFHXOv/ctU7uyKegam5+otX6SqUbN230dcpR2leRNNEKUB5XijrS7d++yadMm1q9fz+XLl3nppZcYN24czzzzTL7nSk9P59KlSzlmx6GhoWg0mhzLFC1btqRx48a6pYoJEybQuXNnpk6dWuj3kZf79+/z7rvvsmfPHtasWZNtF66y3vxFURRu3Lihu9nk+PHjBAUFERsbi5WVFUbdJqJu7prjeWoVhM0bwBK/CEZ2bIipRs2eczf53DdU76x4SLsGLBmZ85uX7L0gKpNKt/dCREQE69evZ8OGDdSoUYOXX36Zl156ibp16xZqbFlLFU8uU4SGhnL79m3s7e1xdHTkwYMHJCQksHTpUlq0aJHrRjpFtXv3bqZMmcKAAQNYuHAh28/dLdUA0mq1XLx4MdsdfUFBQaSmplK7dm1UKhWxsbGo1Wq6dOmCq6srR03acfZezhsWalcz5Z+PPTgVFcek9UGkZ2hZPdaFwEt3WbQnPMfxfRxr88PLHfWOS3YZE5VFpd1lTKvVcujQIdavX4+Pjw9dunRh3LhxvPDCC8UOxseXKg4cOMCmTZto1KiRbqniyWWKli1bUqtWrSK/Xtasd29wOCYDPqAolXL6ftROS0sjNDRUN3vNumW6atWq1KlTB7VazZ07d4iNjcXZ2ZkWLVqg0Wi4f/8+ly9f5urVq9y9e5dq/d7Eok3vHK9paabh1Nz+vPP7CbYcjwbAs3VdpvW2Y9Ay/xzH5zbTzZLfPhhpaWl4Otnq3QdDiPLC4KELhd9cu7AePHjAtm3bWL9+PceOHWPYsGGMGzeO7t27F7t99824B7R5fjLDp8wkKQPUaclUTb+P9f0IrkSczXWpIuv3x5cq8jP4G19CbmWgemy/CBMjNfNfaEM3O2tqVDHhyp0HLNwdzoF/13qzqACXesb00jxqfHn27FkaNGhA3bp1MTIy4t69e1y8eBErKyusrKzQarUkJCRw584dEhMTURSFKlWqULt2bZo3b46TkxPdunXjatUWrAq4pnfJ4MgHvVm0J5ytIXmHrplGzUw9a7r66NsP2KGOBfPGD+Dgnj9LpFmmEKWlXIRulsJsrl1U0dHRbNy4kXXr1pGUlMS4ceMYO3YszZsX7t78xy9kpaSkZOup9uSFLCfb6ty8eTPbMoW+pYrHA9nBwSHbjDy3CgFzYyNe7dkM7+Aoou8n0atFbf4zyhnPbw8RFZeUfdAZabQI24CZKp2YmBjCw8NJSUnB3Nyc9PR0kpOTSUtLw8jIiBo1atCgQQMcHR3p2LEj7u7uODs76/0GkVf1wkwPB9xb2DBx7THSMrSsGdeRwEt3SmVjmhkzZmBtbc2cOXOKfA4hSlu5Ct2ypCgKISEhrF+/nl9//RV7e3tefvllhg8fnu/eDSW5JJKYmEh4eHiOdeMLFy5Qv359XQjfq9uBw3HVKchS7s7pPVi67zy7zsZk+7w2LYX4gF+4H7gFABMTE92stW3btnTr1g13d/dCr39D7qVdGrWKuc+15oVn6pOSnsGO0zf4cmdYtoAuqdKuo0ePMnHiRM6dO1fsn2CEKC1Pbeg+Li0tjV27drF+/Xr27t1L//79GTduHP37989RflZWV9Kzqiqygtg7uiq3quT//FoWJgS835uByw5zIfZBjsftNff4xLMpHTp0yNGmqDjKQ2mXoig0bdqU7du3F6u7hRClKa/QLbXbgMsbY2NjnnvuOX7//XcuXrxIr169WLBgAba2trz99tucOHECyAyWBb5hOQJ3XJfGbH+jG+HzPVnklfOLPSlNywLfME5FxRVoPKmpqZw+fZrDhw+zd+9etm/fTtTNnDtoPUmjVvHtSGe2HI/SG7gAjexa0K1btxINXHjU6l1D4UI38xuSY4lc+FKpVIwaNYrffvut2OcSwhAM377WAKysrJg6dSpTp07l/PnzrF+/nsGDB1O9enWsB39EclrO+/hvxqew/K9IetrbYGas/3tVcnoGKw9E5vgROjExkZMnT+oqCI4ePcqFCxeoUqUKGo2GlJQUkpKSqDGgM/p3n82kUmVuAJ+WoWXO9rO5Hleam780TrtG0pFfsOg5jtQMxSClXaNGjWLo0KEsWLBAlhhEhfPULC/kR6vV8qffQWbsT0Cryr3S4J2+DtSrbsa73qf0Pm5ipOLLLhqOHz3E/v37CQ0NJT4+Ho1Gg1arJT09c/e1qlWrUrduXZo1a4aTkxNdunThkllzVgfeyHW/ioXDnLCtWYXxa//J9RhVRhqdqsTy7qB2tG/fvkT3orhz5w7Ozs7897//pUHbLqVajZIXRVFo2bIl69ato3PnziV6biFKgkE6R1Q0arWaaNNGGBtHFGiTntwkJyczft5GEv7ZikajoWbNmnTs2JHWrVvTuXNn3N3dsbOz09tC6HZiCqsDb+g974LBbbCrbcHoH/7Oc3xGGmOs759nwoSl3Lhxg169euHh4UHfvn1p1qxZkWeGiqIwfvx4Ro4cyYABAwBYNcalTKpRnvT4EoOErqhoJHQfk9/G6wWhNjbluTGv8NPhX3NtVZObWhamuDnY5KgQaFDDnNGdG5OSlsGxjz10n/9422n+OHFd97FKBR6t6rByzNfA19y4cQM/Pz/8/PyYN28epqamugDu3bt3oW7mWLp0Kbdu3WLLli3ZPv9ki6OyMmrUKPr06cOiRYtKfGc5IUqThO5j8tt4vaDMLK0KHbhZ3nC34/D529kqBKLjkmjy0Z/5v67GiNfd7XQf16tXj7FjxzJ27FgURSE0NJS9e/eyYcMGpkyZgp2dnS6Eu3XrlutdfMHBwXz++ecEBgYW+X2VNEdHR2rXro2/vz9ubm6GHo4QBfbUVC8UREE2Xi/YeYp+ISurQsA8l4t1ucmvQkClUtGqVStmzJjB//73P27fvs3SpUsxNzdn7ty51K5dm759+/LVV19x/PhxtNrMGX98fDwjR45kxYoVNGvWrMjvqzSMGjWKX3/91dDDEKJQJHQfk7nxuv6/EiO1ClONGiO1CvVjf35SSXQxGOPahFkDW2JubER+S7AqVWYNbGFrhI2NjenevTuffPIJAQEBREdHM23aNKKjoxk9ejS1a9dm+PDh9O3bl06dOjF8+PBivafSMHLkSLZs2UJamvRHExWHVC88Jq9bXd/qY89bHtnv9//WL4Jv953P9jmNCgI/6kOtasWvkS3t/SryEhUVxZw5c9i6dSumpqZUq1ZNtxTRq1cvrKysSvT1iqpLly7MnTsXT09PQw9FCB25I60QitXFAAX19TPUv/gnn3/+OT169CiRMRmiQuDcuXO4ublx8OBBWrZsyZkzZ/Dz82Pv3r34+/vj6OiIh4cHHh4edOvWDVPTsulg8aSlS5cSEhLC2rVrDfL6QugjoVsIxb3V9ZfJnTh9YAdz5syhdevWLFiwoEQ7TpSFpKQkOnXqxMyZM5k4cWKOx1NTUzl69KiuMuLMmTN07dqVvn374uHhgZOTk96SuNJw48YNWrduzfXr10v8DjwhikpCt5BKYu+FlJQUvv/+ez7//HPc3d2ZP38+dnZ2eZ+knHj11VdJSEhg48aNBarrjYuL48CBA+zduxc/Pz/u3btHnz59dMsRjRo1KtXx9u7dm/FTp/GwjpNB+8gJkUVCtwhKapexxMREli5dypIlS/Dy8mLOnDnUr1+/9AZeTJs3b2bWrFkEBwcXuVvx1atXdbNgPz8/atSooZsF9+rVK98d3grj5LU43lvrx/lEY0xMTAzSR06IJ0noFlFJXsi6c+cOX331FT/88AOTJk3iww8/LDcXo7JcvHgRV1dXdu7cSYcOHUrknFqtltOnT+tmwQEBAbRu3Vo3C+7SpUuRa3913xjTMshrCV7a+4iyJqFbTCV5ISs6Opr58+fj7e3NzJkzmTFjBhYWFqU08oJLTU2le/fujB49mhkzZpTa6yQnJ3P06FFdCIeFhdG9e3ddCLdp06ZASxrSyFKUZxK65dD58+eZM2cOBw4c4OOPP+aVV14xWAUAwLvvvktERAR//PFHme7cdffuXf766y9dZURiYqKuKsLDwwNbW9scz8ntYudvU1xxbliDdG3m/+mY+GT6LD6Y7Rhp2S7KgoRuORYSEsKsWbMIDQ3l008/ZfTo0WW+l8Cff/7Ja6+9RkhICNbW1mX62k+6dOmSbi1437592NjY6GbB7u7uWFpa5lrW99sUV3xCotkUdC3X85dUBwsh8iKbmJdjzs7O+Pr6sn79er7//nueeeYZtm3bRl7fDEtSdHQ0kyZNYuPGjQYPXICmTZsyZcoUNm3axK1bt9i4cSO2trYsW7aMBg0a4OreF79zN4pURw2gKPBXeCx3ElNKduBCFJDMdMsRRVHw9fXl448/xtzcnC+++IJevXqV2utlZGTg4eFB7969mT17dqm9TklJSkpi9i8H8TmfSoaePY9/m+KKfW0LVCoVF2MTWbQnnMBLd3McV5iuxEIUhcx0KwiVSsWgQYMICQlh+vTpTJkyhX79+lFa3/g+++wz1Go1H3/8camcv6SZm5uTblFHb+ACfLkrjJ4L/8L1i338euwqa17uSCOrKjmOS07XEnYjobSHK4ResrVjOaRWq3nppZcYPnw4P/zwAy+88AJdu3Zl/vz5ODo6FugctxNT8A6OyvVmgYMHD7Jq1SqOHz9eIfajvX//PhEREYRGRgP6d3E7ce1Rf7otx6N5/pn69GpRm3VHL+c4Nj5ZNskRhiGhW44ZGxszdepUXlkjCQAABA5JREFUxo0bx7Jly+jRowfPP/88c+fOzfUur5PX4lhxIJKDEbEAT9wsEMMSvwi6NKnOvmWz+Omnn6hXr16ZvJeCSE5O5sKFC0REROT49eDBAxwcHFB1mwAWBdtiUlHIdZe20uwjJ0ReJHQrgCpVqvDBBx/w6quvsnDhQpydnXn55Zf56KOPsLGx0R2X3110WTd3HDh/BxPP97ldo2Cz5pKUkZHBlStXcoTq+fPnuXHjBk2bNsXBwQEHBwc6d+7M2LFjcXBwoF69eqhUKlYdvMASv5wtlSzNNLRrWIO/L90lXavwrFM9OjW1Yt6OcznGUBLbbwpRVHIhrQKKiYnhs88+49dff2XatGm8/fbbbD93t9zcLKAoCjExMZw/fz5HuF66dInatWvrgvXxX40bN0ajyXsekNv2m1ZVTfhpfEea21ig1SpciE3km70R+EfeznEOU42aIx/0lj0ZRKmROt1K6tKlS8ydO5c9weFYPP9/pOdyXbSJdRV2z+iJ75kYZm4+ke2x4twsEBcXpzdYIyIiMDc3zxGq9vb22NnZ5doWqKCKtf2m1OmKMiChW8mNWrGfwGsPQKU/dNdP7ISZxojouKQcoZtfCCUnJxMZGalbAtC3zqovXGvWrFni7zNLcbfflDvSRGmTFuyV2O3EFEJiUnIN3Oec6hGflMbxW/dobF01x+NZNwscPxvBrWsXc8xYY2Jisq2zurq6Mm7cOBwcHKhbt26Z3jKcJauPXNGWU3LvIydEWZDQreC8g6NyfczCVMPMvg6MXvM3I10a5npcUlISQ9/5CvuMq7pwHTRoUIHXWQ0hax26JLbfFKIslb+vJlEoYTHxenu6AbzT14HNx65x435ynudQG5syZMI0loysWB0uxrg2wcm2hsH6yAlRFBK6FVx8crrez7eqZ0k3u1oMWna4gOepmDcLONnWYNUYF4P0kROiKCR0KzhLM/3/hK7NrLCtac6RD3oDUMVEg5FahX3t7jy73F/PeSr2zQLWFqayl4KoECR0KzjHupaYamJyLDH88s9V/nfyhu7jKT2aYVvTnP/740yOc8jNAkKUHdnwpoLz6pBzk2+A5DQtsYkpul8PU9NJSddy90FqjmMVwKu9/vMIIUqWzHQruFoWprg52OR7s8C3+87r/bxKlXmhSdY9hSgbMtOtBN5wt8NMU7Sdwsw0RrzuXjFawwtRGUjoVgJZNwuYGxfun1NuFhCi7MnyQiUhNwsIUTFI6FYicrOAEOWfhG4lIzcLCFG+SehWUnKzgBDlk1xIE0KIMiShK4QQZUhCVwghylCenSNUKlUscKXshiOEEJVCY0VRbPQ9kGfoCiGEKFmyvCCEEGVIQlcIIcqQhK4QQpQhCV0hhChDErpCCFGG/h9uoOtzWT6YXQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def test_load():\n",
        "    network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n",
        "    nx.draw_networkx(network,font_color='white')\n",
        "    plt.show()\n",
        "\n",
        "test_load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UTIxaGIXGzz"
      },
      "source": [
        "### 1.2  Modularit√© / Modularity (1 point)\n",
        "\n",
        "La modularit√© $Q$ du r√©seau est une mesure importante pour l'algorithme: elle permet de savoir si l'algorithme a atteint un optimum local. $$ Q=\\frac{1}{2m}\\sum_{u,v=1}^n B_{uv}\\delta(l_u,l_v)$$ \n",
        "\n",
        "- m: le nombre d'ar√™tes\n",
        "- l: l'√©tiquette du sommet\n",
        "- u, v: des sommets dans le r√©seau\n",
        "- B: la matrice de modularit√© o√π chaque √©l√©ment vaut $A_{uv} - P_{uv}$\n",
        "- $A_{uv}$: vaut 1 si il y une ar√™te entre u et v sinon 0\n",
        "- $P_{uv}$: la probabilit√© qu'il y ait une ar√™te entre u et v selon le mod√®le nul  $$P_{uv}=\\frac{degree(u)*degree(x)}{2m}$$\n",
        "- $\\delta(l_u,l_v)$: delta de Kronecker, vaut 1 si les deux labels sont identiques sinon 0\n",
        "\n",
        "Elle peut aussi √™tre d√©finie comme: $$Q=\\sum_{t=1}^{N_c}\\left(\\frac{I_t}{m}-\\left(\\frac{D_t}{2m}\\right)^2\\right)$$\n",
        "\n",
        "- m: le nombre d'ar√™tes\n",
        "- Nc: le nombre de communaut√©s\n",
        "- t: une communaut√© dans le r√©seau\n",
        "- $I_t$: le nombre d'ar√™tes dans la communaut√© t c'est-√†-dire que les deux sommets de l'ar√™te appartiennent √† t\n",
        "- $D_t$: la somme des degr√©s de tous les sommets appartenant √† t\n",
        "\n",
        "#### Impl√©mentation\n",
        "1. Impl√©mentez  la fonction  `modularity`  dans LPAmPlus. Cette fonction retourne la modularit√© du r√©seau. Vous pouvez utiliser la fonction `linalg.modularity_matrix` de networkx pour calculer la matrice B. Prenez la d√©finition pr√©sent√©e que vous voulez. **N.B:** Networkx permet d'ajouter du data sur les sommets pour garder des informations sur le node. Les nodes agissent comme des dictionnaires.\n",
        "\n",
        "Utilisez la fonction `test_modularity` pour v√©rifier votre impl√©mentation de la fonction. Vous devriez obtenir une modularit√© d'environ 0.413.\n",
        "\n",
        "---\n",
        "\n",
        "The modularity $Q$ of the network is an important measure for the algorithm. The algorithm uses it to determine if it reached a local optimum or not. $$ Q=\\frac{1}{2m}\\sum_{u,v=1}^n B_{uv}\\delta(l_u,l_v)$$ \n",
        "\n",
        "- m: number of edges\n",
        "- l: node's label\n",
        "- u, v: nodes in the graph\n",
        "- B: modularity matrix where each element is $A_{uv} - P_{uv}$\n",
        "- $A_{uv}$: is 1 if there is an edge between u and v else 0\n",
        "- $P_{uv}$: probability that there is an edge between u and v following the null model $$P_{uv}=\\frac{degree(u)*degree(x)}{2m}$$\n",
        "- $\\delta(l_u,l_v)$: Kronecker's delta, is 1 if labels are the same else 0\n",
        "\n",
        "The modularity can also be defined like this: $$Q=\\sum_{t=1}^{N_c}\\left(\\frac{I_t}{m}-\\left(\\frac{D_t}{2m}\\right)^2\\right)$$\n",
        "\n",
        "- m: number of edges\n",
        "- Nc: the number of community in the graph\n",
        "- t: a community in the graph\n",
        "- $I_t$: the number of arc in the community t meaning all arcs that have both nodes in the community t\n",
        "- $D_t$: the sum of degree of all the nodes in the community t\n",
        "\n",
        "#### Implementation\n",
        "1. Implement the function `modularity` in the class LPAmPlus. This function returns the modularity of the network. You can use the function `linalg.modularity_matrix` from networkx to calculate B. You can implement whichever definition for the modularity. **N.B:** You can add data to nodes with Networkx to store information about the node. You can add data to nodes with Networkx to store information about the node. The nodes act like a dictionnary.\n",
        "\n",
        "Use the function `test_modularity` to test your implementation. You should have a modularity of 0.413."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7szMoEQSriF",
        "outputId": "62d87cae-213b-44ee-e749-162aaad025d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modularity: 0.41265306122448997\n"
          ]
        }
      ],
      "source": [
        "def test_modularity():\n",
        "    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n",
        "    lpam = LPAmPlus(social_network)\n",
        "    lpam.labels = [0, 1]\n",
        "    for i in [0,1,2,3,4,5,6,7,8,9]:\n",
        "        lpam.graph.nodes[i]['label'] = 0\n",
        "    for i in [10,11,12,13,14,15]:\n",
        "        lpam.graph.nodes[i]['label'] = 1\n",
        "    print(\"Modularity: {}\".format(lpam.modularity()))\n",
        "\n",
        "test_modularity()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwxvqYj4XGzu"
      },
      "source": [
        "### 1.3 R√®gle de modification des √©tiquettes / Updating rule for the labels (1.5 point)\n",
        "\n",
        "Comme mentionn√© plus haut, l'algorithme est fortement bas√© sur son optimisation de la modularit√©. Il vous est maintenant demand√© d'impl√©menter le terme √† optimiser. La nouvelle √©tiquette $l_x^{new}$ correspond √† l'√©tiquette pour laquelle la somme donne la plus grande valeur.\n",
        "$$l_x^{new}=\\arg\\max_l\\sum_{u=1}^n B_{ux}\\delta(l_u,l)$$\n",
        "\n",
        "- n: le nombre de sommets\n",
        "- m: le nombre d'ar√™tes\n",
        "- l: une √©tiquette possible pour le sommet x\n",
        "- x: le sommet qu'on √©value en ce moment\n",
        "- u: un autre sommet dans le r√©seau (commence √† 1, car on exclut le sommet x)\n",
        "- B: la matrice de modularit√© o√π chaque √©l√©ment vaut $A_{ux} - P_{ux}$\n",
        "- $A_{ux}$: vaut 1 si il y une ar√™te entre u et x sinon 0\n",
        "- $P_{ux}$: la probabilit√© qu'il y ait une ar√™te entre u et x selon le mod√®le nul  $$P_{ux}=\\frac{degree(u)*degree(x)}{2m}$$\n",
        "- $\\delta(l_u,l)$: delta de Kronecker, vaut 1 si les deux labels sont identiques sinon 0\n",
        "\n",
        "\n",
        "#### Impl√©mentation\n",
        "1. Impl√©menter la fonction `label_evaluation`. Cette fonction retourne la valeur du terme √† optimiser. Vous pouvez utiliser la fonction `linalg.modularity_matrix` de networkx pour calculer la matrice B. Il est normal qu'il y ait une ressemblance avec le calcul de la modularit√© selon la d√©finition que vous avez prise. `new_label` correspond donc √† un $l$ possible dans le terme.\n",
        "2. Impl√©menter la fonction `update_label`. Cette fonction choisit la nouvelle √©tiquette pour le sommet actuel. En cas d'√©galit√©, la fonction choisit une √©tiquette au hasard parmi les meilleurs. N'oubliez pas d'enlever les √©tiquettes d√©su√®tes du param√®tre `labels`. **N.B:** Il est possible que la meilleure √©tiquette soit celle actuelle du sommet.\n",
        "\n",
        "Networkx permet d'ajouter du data sur les sommets. Les sommets sont des dictionnaires dans le graphe.\n",
        "\n",
        "---\n",
        "\n",
        "As mentioned above, the algorithm is strongly based on its optimization of modularity. You are now asked to implement the term to optimize. The new label $l_x^{new}$ corresponds to the label for which the sum gives the greatest value.\n",
        "$$l_x^{new}=\\arg\\max_l\\sum_{u=1}^n B_{ux}\\delta(l_u,l)$$\n",
        "\n",
        "- n: number of nodes\n",
        "- m: number of edges\n",
        "- l: a possible label for the node x\n",
        "- x: current node being evaluated\n",
        "- u: another node in the network (starts at 1, because we exclude the node x)\n",
        "- B: modularity matrix where each element is $A_{ux} - P_{ux}$\n",
        "- $A_{ux}$: is 1 if there is an edge between u and x else 0\n",
        "- $P_{ux}$: the probability that there is an edge between u and x  following the null model  $$P_{ux}=\\frac{degree(u)*degree(x)}{2m}$$\n",
        "- $\\delta(l_u,l)$: Kronecker's delta, is 1 if labels are the same else 0\n",
        "\n",
        "\n",
        "#### Implementation\n",
        "1. Implement the function `label_evaluation`. This function returns the value for the term to optimize. You can use the function `linalg.modularity_matrix` from networkx to calculate B. It is normal if there is a similarity with the modularity depending on the definition you took. `new_label` represent a possible $l$ in the term.\n",
        "2. Implement the function `update_label`. This function chooses the new label for the current node. If there is more than one label with the max value, the function chooses randomly one amoung those. Don't forget to remove the unused labels from the `labels` attribute. **N.B:** The best label can be the node's current label. \n",
        "\n",
        "You can add data to nodes with Networkx to store information about the node. The nodes act like a dictionnary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5_N31uPXGz2"
      },
      "source": [
        "### 1.4 LPAm (2 points)\n",
        "\n",
        "Vous pouvez maintenant impl√©menter l'algorithme LPAm. Cet algorithme est le pr√©d√©cesseur de LPAm+ puisque LPAm+ a √©t√© cr√©e pour contourner une faiblesse de LPAm.  LPAm est un algorithme de propapagation d'√©tiquettes bas√© sur la modularit√©. Il commence par donner une √©tiquette unique √† chaque sommet. Il explore par la suite tous les sommets et change leur √©tiquette selon la fonction d'√©valuation que vous avez impl√©ment√©e plus t√¥t. L'algorithme continue la propagation d'√©tiquette √† travers tous les sommets jusqu'√† un optimun de la modularit√©.\n",
        "\n",
        "#### Impl√©mentation\n",
        "1. Ajouter les √©tiquettes initiales aux sommets du graphe dans la fonction `__init__`. Il faut que chaque sommet soit dans sa propre communaut√© au d√©but de l'algorithme. Initialiser le param√®tre `labels` pour qu'il contient la liste des √©tiquettes pr√©sentes dans le r√©seau.\n",
        "\n",
        "2. Impl√©menter l'algorithme LPAm dans la fonction `LPAm`. Assurez-vous de toujours augmenter la modularit√© lors de vos changements d'√©tiquettes. N'oubliez pas de garder le param√®tre `labels` √† jour √† fur et √† mesure lors de vos changements pour ne pas √©valuer plusieurs fois la m√™me √©tiquette.\n",
        "\n",
        "Utilisez la fonction `test_lpam` pour v√©rifier votre impl√©mentation. Vous devriez finir avec une modularit√© d'environ 0.399 avec 4 communaut√©s.\n",
        "\n",
        "---\n",
        "\n",
        "You can now implement the LPAm algorithm. This algorithm is the predecessor of LPAm+ since LPAm+ was created to overcome LPAm's weakness. LPAm is a label probagation algorithm based on modularity. It begins by giving a unique label to each node. It then explores all the nodes and changes their label according to the evaluation function that you implemented earlier. The algorithm continues until it can no longer improve the modularity of the network.\n",
        "\n",
        "#### Implementation\n",
        "1. Add the initial labels to the nodes in the graph in the function `__init__`. Each nodes has to be in their own community in the beginning. Initialise `labels` with the current list of labels present in the graph.\n",
        "\n",
        "2. Implement the LPAm algorithm in the function`LPAm`. Make sure that all your labels changes improve the modularity. Don't forget to keep your `labels` parameter is kept up-to-date so that you dont evaluate the same label multiple times or unused labels.\n",
        "\n",
        "Use the function `test_lpam` to verify your implementation. You should have a modularity of 0.399 with 4 communities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2RfrO6PSriL",
        "outputId": "67034509-25d8-4e96-bab0-ad2047a00cdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modularity: 0.39877551020408175\n",
            "Communities: [3, 5, 8, 11]\n"
          ]
        }
      ],
      "source": [
        "def test_lpam():\n",
        "    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n",
        "    lpam = LPAmPlus(social_network)\n",
        "    lpam.LPAm()\n",
        "    print(\"Modularity: {}\\nCommunities: {}\".format(lpam.modularity(), lpam.labels))\n",
        "\n",
        "test_lpam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj5Ghd5jXGz6"
      },
      "source": [
        "### 1.5 LPAm+ (2 point)\n",
        "\n",
        "Vous pouvez maintenant impl√©menter LPAm+ au complet. LPAm+ est une am√©lioration de LPAm. Lorsque LPAm tombe dans un optimum local, LPAm+ essaye de combiner deux communaut√©s pour augmenter la modularit√© et ainsi sortir du optimum local. LPAm+ choisit la combinaison qui augmente le plus la modularit√© et recommence la propagation d'√©tiquette jusqu'au prochain optimum local o√π il va reessayer de combiner des communaut√©s. L'algorithme continue jusqu'√† qu'il ne peut plus augmenter la modularit√©.\n",
        "\n",
        "#### Impl√©mentation\n",
        "1. Impl√©mentez  la fonction  `merge_communities`. Cette fonction regarde si combiner des communaut√©s augmente la modularit√© et combine le meilleur choix. Elle retourne True si une combinaison a √©t√© faite sinon False (aucune combinaison augmente la modularit√©).\n",
        "2. Impl√©menter `find_communities`. Cette fonction applique l'algorithme LPAm+ sur le r√©seau en utilisant les fonctions `LPAm` et `merge_communities`.\n",
        "\n",
        "Utilisez la fonction `test_lpam_plus` pour v√©rifier votre impl√©mentation. Vous devriez finir avec une modularit√© d'environ 0.413 et 2 communaut√©s.\n",
        "\n",
        "---\n",
        "\n",
        "You can now fully implement LPAm+. As said before LPAm+ is an amelioration of LPAm. The issue with LPAm is that it stops when it finds a local optimun. To prevent that, LPAm+ tries to combine two communities to increase modularity and escape the local optimun. LPAm+ chooses the combination that most increases modularity and restart the label's propagation until the next local optimum where it will try to combine two communities again. The algorithm continues until it can no longer increase modularity.\n",
        "\n",
        "#### Implementation\n",
        "1. Implement the function  `merge_communities`. This function check if combining communities improve the modularity and combine the best choice. It returns True if a combinaison was made else False (no combination increase the modularity).\n",
        "2. Implement the LPAM+ algorithm in the function `find_communities` using the fonctions `LPam` and `merge_communities`.\n",
        "\n",
        "Use the function `test_lpam_plus` to verify your implementation. You should end with a modularity of 0.413 and 2 communities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVSnUWQoSriP",
        "outputId": "c30fc5eb-c878-441d-aa5d-bd14c71be758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modularity: 0.41265306122448997\n",
            "Communities: [3, 11]\n"
          ]
        }
      ],
      "source": [
        "def test_lpam_plus():\n",
        "    social_network = load_unweighted_network(\"data/toy-nodes.csv\", \"data/toy-edges.csv\")\n",
        "    lpam = LPAmPlus(social_network)\n",
        "    lpam.find_communities()\n",
        "    print(\"Modularity: {}\\nCommunities: {}\".format(lpam.modularity(), lpam.labels))\n",
        "\n",
        "test_lpam_plus()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vODCJbRaXGz-"
      },
      "source": [
        "### 1.6 GOT dataset (3 points)\n",
        "\n",
        "Rouler votre algorithme sur les donn√©es de Games of Thrones de chaque saison et comparer ce que vous obtenez et les vraies communaut√©s. Le ground truth se trouve dans la colonne Community des csv. Des liens sont pr√©sents entre des personnages lorsque: \n",
        "- Personnage A parle directement apr√®s Personnage B\n",
        "- Personnage A parle de Personnage B\n",
        "- Personnage C parle de Personnage A et Personnage B\n",
        "- Personnage A et Personnage B font une action ensemble dans une sc√®ne (ex: quittent les lieux, A regarde B, sont assis √† une table, etc)\n",
        "- Personnage A et Personnage B apparaissent ensemble dans une sc√®ne\n",
        "\n",
        "Commencez par calculer le ARI (ajusted Rand index) de vos r√©sultats. $$ ARI=\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{TP+TN}{\\binom{n}{2}}$$\n",
        "\n",
        "- n: le nombre de sommets\n",
        "- TP: True positive soit le nombre de paires d'√©l√©ments qui se trouvent dans la m√™me communaut√© dans vos r√©sultats et dans le ground truth\n",
        "- TN: True n√©gative soit le nombre de paires d'√©l√©ments qui se trouvent dans des communaut√©s diff√©rentes dans vos r√©sultats et dans le ground truth\n",
        "- FP: False positive soit le nombre de paires d'√©l√©ments qui se trouvent dans la m√™me communaut√© dans vos r√©sultats mais qui sont dans des communaut√©s diff√©rentes dans le ground truth\n",
        "- FN: False n√©gative soit le nombre de paires d'√©l√©ments qui se trouvent dans des communaut√©s diff√©rentes alors qu'ils sont dans la m√™me communaut√© dans le ground truth\n",
        "\n",
        "\n",
        "**N.B**: Ce n'est pas le nom des communaut√©s que vous avez trouv√© qui importante mais leur composition. Autrement dit, un TP est si le sommet a et le sommet b se trouve dans la m√™me communaut√© dans vos r√©sultats et dans le ground truth.\n",
        "\n",
        "\n",
        "R√©pondez aux questions suivantes. Elles servent comme piste de r√©flexion pour votre analyse.\n",
        "\n",
        "- L'algorithme performe-t-il bien sur toutes les saisons ou pour certaines seulement? \n",
        "- Expliquez pourquoi vous avez obtenu ces r√©sultats en analysant la formation des communaut√©s dans chaque saison. Quelles particularit√©s favorisent des bons r√©sultats? Quelles particularit√©s nuisent √† l'algorithme?\n",
        "\n",
        "Vous pouvez faire les manipulations que vous voulez pour mieux pr√©senter vos r√©sultats et mieux appuyer vos affirmations. \n",
        "\n",
        "---\n",
        "\n",
        "Run your algorithm over the Games of Thrones data from each season and compare what you get and the real communities. The ground truth is found in the Community column in the csv. Links are found between characters A and B when:\n",
        "- Character A talks directly after Character B\n",
        "- Character A talks about Character B\n",
        "- Character C talks about Character B and A\n",
        "- Character A and Character B does an action together in a scene (ex: leave the room, A looks toward B, are seated together at a table, etc)\n",
        "- Character A and Character B are both present in a scene\n",
        "\n",
        "Start by calculating the ARI (adjusted Rand index) of your results. $$ ARI=\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{TP+TN}{\\binom{n}{2}}$$\n",
        "\n",
        "- n: number of nodes\n",
        "- TP: True positive the number of pairs of elements that are in the same community in your results and in the ground truth\n",
        "- TN: True negative the number of pairs of elements that are in different communities in your results and in the ground truth\n",
        "- FP: False positive the number of pairs of elements which are in the same community in your results but which are in different communities in the ground truth\n",
        "- FN: False negative the number of pairs of elements which are in different communities in your results but which are in the same community in the ground truth\n",
        "\n",
        "**N.B:** What matters here is the composition of the communities you found not the names. A TP is when the node a and the node b are both in the same communities in your result and in the ground truth.\n",
        "\n",
        "Answer the following questions. They are guides for your analysis.\n",
        "\n",
        "- Does the algorithm perform well on all seasons or for some only? \n",
        "- Explain why you obtained those results by analysing the communities from each season. Which particularities offer better results? Which hinder the algorithm?\n",
        "\n",
        "You can do the manipulations you want to better present your results and better support your statements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzndRBWVSriT"
      },
      "source": [
        "#### R√©sultats / Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-2UipMRXG0R",
        "outputId": "66e6ca17-9c55-478e-c833-6873c260aeba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------Season 1------------------ \n",
            "\n",
            "Modularity:  0.4531089810584543\n",
            "Communities:  ['Lannister', 'Night Watch', \"King's Landing\", 'Stark', 'Dothraki', 'Doomed Night Watch', 'Orphan']\n",
            "TP:  1371 TN:  5928 FP:  298 FN:  278\n",
            "ARI:  0.9268571428571428\n",
            "Precision:  0.8214499700419413\n",
            "Recall:  0.8314129775621589\n",
            "--------------------Season 2------------------ \n",
            "\n",
            "Modularity:  0.5692983793120965\n",
            "Communities:  ['Harenhall', 'Riverlands and Stormlands', 'Iron Islands and Winterfell', \"King's Landing\", 'Far North', 'Red Waste and Qarth', 'Janos']\n",
            "TP:  1383 TN:  6672 FP:  115 FN:  86\n",
            "ARI:  0.9756540697674418\n",
            "Precision:  0.9232309746328438\n",
            "Recall:  0.9414567733151804\n",
            "--------------------Season 3------------------ \n",
            "\n",
            "Modularity:  0.6273571054427709\n",
            "Communities:  ['Dragonstone', \"Kingslayer's trek\", \"Craster's keep\", \"Theon's imprisonment\", 'Accross the Narrow Sea', 'Stark Bannermen', \"King's Landing\", 'The North', 'Beyond the Wall']\n",
            "TP:  1009 TN:  6097 FP:  391 FN:  6\n",
            "ARI:  0.9470878315340531\n",
            "Precision:  0.7207142857142858\n",
            "Recall:  0.994088669950739\n",
            "--------------------Season 4------------------ \n",
            "\n",
            "Modularity:  0.6071679787895999\n",
            "Communities:  ['Winterfell', \"King's Landing\", 'Castle Black', 'Eyrie', 'Riverlands', 'Dragonstone', 'Dwarf Troup', 'Meereen', 'Beyond the wall']\n",
            "TP:  1843 TN:  11510 FP:  827 FN:  355\n",
            "ARI:  0.9186790505675955\n",
            "Precision:  0.6902621722846441\n",
            "Recall:  0.8384895359417652\n",
            "--------------------Season 5------------------ \n",
            "\n",
            "Modularity:  0.6701165697377894\n",
            "Communities:  [\"Night's Watch\", 'Essos', 'Dorne', 'Braavos', 'Winterfell', \"King's Landing\"]\n",
            "TP:  1263 TN:  5623 FP:  41 FN:  94\n",
            "ARI:  0.9807719698048711\n",
            "Precision:  0.968558282208589\n",
            "Recall:  0.9307295504789977\n",
            "--------------------Season 6------------------ \n",
            "\n",
            "Modularity:  0.655484981942791\n",
            "Communities:  ['Ironborn', 'Essos', 'North', 'Dorne', 'Beyond the wall', \"King's Landing\", 'Braavos', 'Riverlands', 'Oldtown']\n",
            "TP:  1269 TN:  8346 FP:  172 FN:  224\n",
            "ARI:  0.9604435121366497\n",
            "Precision:  0.8806384455239417\n",
            "Recall:  0.8499665103817816\n",
            "--------------------Season 7------------------ \n",
            "\n",
            "Modularity:  0.3211394806296525\n",
            "Communities:  ['Dragonstone', 'Winterfell', \"King's Landing\", 'Northen Expedition']\n",
            "TP:  587 TN:  2289 FP:  126 FN:  238\n",
            "ARI:  0.8876543209876543\n",
            "Precision:  0.82328190743338\n",
            "Recall:  0.7115151515151515\n",
            "--------------------Season 8------------------ \n",
            "\n",
            "Modularity:  0.2088182492993989\n",
            "Communities:  ['Stark/Targaryen', 'Citizen pair 1', \"King's Landing\", 'Other', 'Citizen pair 2']\n",
            "TP:  494 TN:  964 FP:  128 FN:  1115\n",
            "ARI:  0.5398000740466494\n",
            "Precision:  0.7942122186495176\n",
            "Recall:  0.3070229956494717\n"
          ]
        }
      ],
      "source": [
        "# Mettez votre code ici\n",
        "\"\"\"\n",
        "Rouler votre algorithme sur les donn√©es de Games of Thrones de chaque saison \n",
        "    et comparer ce que vous obtenez et les vraies communaut√©s. \n",
        "Le ground truth se trouve dans la colonne Community des csv.\n",
        "\"\"\"\n",
        "# on s'attend pas √† un ARI de 100%\n",
        "for season in range(1,9):\n",
        "    print(\"--------------------Season {}------------------ \\n\".format(season))\n",
        "    got_nodes = \"data/got-s{}-nodes.csv\".format(season)\n",
        "    got_edges = \"data/got-s{}-edges.csv\".format(season)\n",
        "    social_network = load_unweighted_network(got_nodes, got_edges)\n",
        "    lpam = LPAmPlus(social_network)\n",
        "    lpam.find_communities()\n",
        "    nodes = lpam.graph.nodes\n",
        "    true_labels = list(dict.fromkeys(list(nx.get_node_attributes(lpam.graph, 'ground_truth').values())))\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    for nodes_combination in combinations(list(nodes), 2):\n",
        "        node_0_label = nodes[nodes_combination[0]]['label']\n",
        "        node_1_label = nodes[nodes_combination[1]]['label']\n",
        "        node_0_true_label = nodes[nodes_combination[0]]['ground_truth']\n",
        "        node_1_true_label = nodes[nodes_combination[1]]['ground_truth']\n",
        "        if    (node_0_label == node_1_label) and (node_0_true_label == node_1_true_label):\n",
        "            TP +=1\n",
        "        elif  (node_0_label != node_1_label) and (node_0_true_label != node_1_true_label):\n",
        "            TN +=1\n",
        "        elif  (node_0_label == node_1_label) and (node_0_true_label != node_1_true_label):\n",
        "            FP +=1\n",
        "        else:\n",
        "            FN +=1\n",
        "    print(\"Modularity: \", lpam.modularity())\n",
        "    print(\"Communities: \", true_labels)\n",
        "    print(\"TP: \",TP, \"TN: \", TN, \"FP: \", FP, \"FN: \", FN )\n",
        "    print(\"ARI: \", (TP+TN)/(TP+TN+FP+FN))\n",
        "    print(\"Precision: \", TP/(TP+FP))\n",
        "    print(\"Recall: \", TP/(TP+FN))\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-HMg4sUSriX"
      },
      "source": [
        "#### Analyse / Analysis\n",
        "\n",
        "1.6.1. L'algorithme performe-t-il bien sur toutes les saisons ou pour certaines seulement?\n",
        "\n",
        "L'algorithme semble avoir bien fonctionner sur les sept premi√®res saisons.\n",
        "\n",
        "1.6.2. Expliquez pourquoi vous avez obtenu ces r√©sultats en analysant la formation des communaut√©s dans chaque saison. Quelles particularit√©s favorisent des bons r√©sultats? Quelles particularit√©s nuisent √† l'algorithme\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3FmllqgXG0d"
      },
      "source": [
        "# 2. Personnages influents dans GOT / Influent character in GOT (4 points)\n",
        "\n",
        "##  Analyse d'un r√©seau social \n",
        "Une autre analyse int√©ressante √† faire avec un r√©seau social est de trouver les personnes influentes du r√©seau soit les personnes autour desquelles les gens du r√©seau se regroupent.\n",
        "\n",
        "Il existe des mesures qui permettent de conna√Ætre ces personnes: les mesures de centralit√©. **Vous devez impl√©menter les mesures vous-m√™me et ne pas utilisez les impl√©mentations de networks de ces mesures.** Pour vous aider lors de l'impl√©mentation de ses mesures, un deuxi√®me toy dataset vous est fourni. Il ressemble √† ceci:\n",
        "![title](data/picture2.png)\n",
        "\n",
        "## GOT datasets\n",
        "La s√©rie Games of Thrones est reconnue pour tuer ses personnages importants. Nous vous demandons de v√©rifier cette affirmation. Pour cette partie, vous devez utiliser tous les CSV donn√©s avec le TP (nodes, edges et deaths). Nous voulons que vous trouviez les personnages les plus influents de chaque saison et que vous les compariez avec la liste de personnages morts durant la saison.\n",
        "\n",
        "---\n",
        "\n",
        "##  Social network analysis\n",
        "\n",
        "Another interesting analysis to do with a social network is to find the influential people in the network, ie the people around whom the people in the network gather.\n",
        "\n",
        "There are measures which make it possible to know these people: the centrality measures. **You must implement those metrics yourselves. Do not use Networks implementation for the  tp.** To help you during the implementation of those measurements, a second toy dataset is provided to you. It looks like this: ![title](data/picture2.png)\n",
        "\n",
        "## GOT datasets\n",
        "The Games of Thrones series is known to kill its important characters. We ask you to verify this statement. For this part, you must use all the csv given with the TP (nodes, edges and deaths). We want you to find the most influential characters from each season and compare them with the list of dead characters during the season.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJxSGCnOXG0e"
      },
      "source": [
        "## 2.1 Centralit√© de degr√© / Degree centrality (0.5 point)\n",
        "\n",
        "Une premi√®re mesure simple pour trouver l'importance d'un sommet dans un r√©seau est la centralit√© de degr√©. Elle se calcule $$C_{D}(i) = \\frac{degree(i)}{n-1}$$\n",
        "\n",
        "- i: un sommet dans le r√©seau\n",
        "- n: le nombre de sommets\n",
        "- degree: le nombre d'ar√™tes attach√©es au sommet\n",
        "\n",
        "#### Impl√©mentation\n",
        "1. Impl√©menter la fonction `calculate_degree_centrality`. Cette fonction calcule la centralit√© de degr√© pour tous les sommets du r√©seau et ajoute cette mesure √† chaque sommet.\n",
        "\n",
        "Utilisez la fonction `test_degree_centrality` pour v√©rifier votre impl√©mentation. Le sommet 1 devrait avoir la plus haute mesure de 0.4375.\n",
        "\n",
        "---\n",
        "\n",
        "A first simple measure to find the importance of a node in a network is the degree centrality. It is calculated $$C_{D}(i) = \\frac{degree(i)}{n-1}$$\n",
        "\n",
        "- i: a node in the network\n",
        "- n: the number of nodes\n",
        "- degree: the number of edges attached to the node\n",
        "\n",
        "#### Implementation\n",
        "1. Implement the function `calculate_degree_centrality`. This function calculates degree centrality for all nodes in the network and adds this measurement to each node.\n",
        "\n",
        "Use the function `test_degree_centrality` to verify your implementation. The best node should be node_1 with 0.4375."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLTg0prlSriZ"
      },
      "outputs": [],
      "source": [
        "def calculate_degree_centrality(social_network):\n",
        "    #TODO\n",
        "    n = social_network.number_of_nodes()\n",
        "    for node in social_network.nodes():\n",
        "        degree_i = sum(1 for i in social_network.neighbors(node))\n",
        "        social_network.nodes[node]['degree_centrality'] = degree_i / (n - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad3lK2B8Srie",
        "outputId": "1e0a33e6-375c-40e7-f322-c0b187350782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Highest degree centrality node: node_1 with 0.4375\n"
          ]
        }
      ],
      "source": [
        "def test_degree_centrality():\n",
        "    social_network = load_unweighted_network(\"data/toy-nodes2.csv\", \"data/toy-edges2.csv\")\n",
        "    calculate_degree_centrality(social_network)\n",
        "    dict_centrality = nx.get_node_attributes(social_network, 'degree_centrality')\n",
        "    best_node = max(dict_centrality, key=dict_centrality.get)\n",
        "    print(\"Highest degree centrality node: {} with {}\".format(best_node, dict_centrality[best_node]))\n",
        "test_degree_centrality()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhPWALjLSrig"
      },
      "source": [
        "## 2.2 Centralit√© de proximit√© / Closeness centrality (0.5 point)\n",
        "\n",
        "Une autre mesure simple pour trouver l'importance d'un sommet dans un r√©seau est la centralit√© de proximit√©. Elle se calcule $$C_{P}(i) = \\frac{1}{AvDist(i)}$$\n",
        "\n",
        "- i: un sommet dans le r√©seau\n",
        "- AvDist: la moyenne de toutes les distances les plus courtes pour atteindre chaque sommet √† partir du sommet i\n",
        "\n",
        "#### Impl√©mentation\n",
        "1. Impl√©menter la fonction `calculate_closeness_centrality`. Cette fonction calcule la centralit√© de proximit√© pour tous les sommets du r√©seau et ajoute cette mesure √† chaque sommet. Consid√©rer chaque ar√™te comme une distance de 1.\n",
        "\n",
        "**NB**: Utiliser la fonction `shortest_path()` du module Networkx pour trouver le chemin le plus court entre des sommets\n",
        "\n",
        "Utilisez la fonction `test_closeness_centrality` pour v√©rifier votre impl√©mentation. Le sommet 3 devrait avoir la plus haute mesure de 0.41.\n",
        "\n",
        "---\n",
        "\n",
        "Another simple measure for finding the importance of a node in a network is closeness centrality. It is calculated $$C_{P}(i) = \\frac{1}{AvDist(i)}$$\n",
        "\n",
        "- i: a node in the network\n",
        "- AvDist: the average of all shortest distances to reach each vertex from vertex i\n",
        "\n",
        "#### Implementation\n",
        "1. Implement the function `calculate_closeness_centrality`. This function calculates closeness centrality for all nodes in the network and adds this measurement to each node. Consider each edge as a distance of 1.\n",
        "\n",
        "**NB**: Use the fucntion `shortest_path()` from Networkx to find the shortest path between two nodes.\n",
        "\n",
        "Use the function `test_closeness_centrality` to verify your implementation. The best node should be node_3 with 0.41."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "534teZVYSrii"
      },
      "outputs": [],
      "source": [
        "def calculate_closeness_centrality(social_network):\n",
        "    #TODO\n",
        "    for node in social_network.nodes():\n",
        "        p = nx.shortest_path(social_network, source = node)\n",
        "        AvDist_i = sum(len(p[k]) - 1 for k in p) / (len(p) - 1)\n",
        "        social_network.nodes[node]['closeness_centrality'] = 1 / AvDist_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVQG0XiFSril",
        "outputId": "2e57331e-b262-4be0-f7fe-f484f5b5dc34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Highest closeness centrality node: node_3 with 0.41025641025641024\n"
          ]
        }
      ],
      "source": [
        "def test_closeness_centrality():\n",
        "    social_network = load_unweighted_network(\"data/toy-nodes2.csv\", \"data/toy-edges2.csv\")\n",
        "    calculate_closeness_centrality(social_network)\n",
        "    dict_centrality = nx.get_node_attributes(social_network, 'closeness_centrality')\n",
        "    best_node = max(dict_centrality, key=dict_centrality.get)\n",
        "    print(\"Highest closeness centrality node: {} with {}\".format(best_node, dict_centrality[best_node]))\n",
        "\n",
        "test_closeness_centrality()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHTdy58OSrir"
      },
      "source": [
        "## 2.3 Centralit√© d'interm√©diarit√© / Betweeness centrality (1 point)\n",
        "\n",
        "Une derni√®re mesure simple pour trouver l'importance d'un sommet dans un r√©seau est la centralit√© d'interm√©diarit√©. Elle se calcule $$C_{I}(i) = \\frac{\\sum_{j<k}f_{jk}(i)}{\\binom{n}{2}}$$\n",
        "\n",
        "- n: le nombre de sommets dans le r√©seau\n",
        "- i: un sommet dans le r√©seau\n",
        "- j,k: deux sommets dans le r√©seau excluant i\n",
        "- $f_{jk}(i)$: le nombre de chemin le plus court partant du sommet j vers un sommet k (> j) passant par le sommet i \n",
        "\n",
        "#### Impl√©mentation\n",
        "1. Impl√©menter la fonction `calculate_betweenness_centrality`. Cette fonction calcule la centralit√© d'interm√©diarit√© pour tous les sommets du r√©seau et ajoute cette mesure √† chaque sommet.\n",
        "\n",
        "Utilisez la fonction `test_betweennes_centrality` pour v√©rifier votre impl√©mentation. Le sommet 4 devrait avoir la plus haute mesure de 0.57.\n",
        "\n",
        "---\n",
        "\n",
        "A final simple measure to find the importance of a node in a network is the betweeness centrality. It is calculated $$C_{I}(i) = \\frac{\\sum_{j<k}f_{jk}(i)}{\\binom{n}{2}}$$\n",
        "\n",
        "- n: the number of nodes in the network\n",
        "- i: a node in the network\n",
        "- j,k: two nodes in the network excluding i\n",
        "- $f_{jk}(i)$: the  number of shortest paths from vertex j to vertex k (> j) passing through node i\n",
        "\n",
        "#### Implementation\n",
        "1. Implement the function `calculate_betweenness_centrality`.This function calculates the betweenness centrality for all the nodes of the network and adds this measurement to each node.\n",
        "\n",
        "Use the function `test_betweennes_centrality` to verify your implementation. The best node should be the node_4 with 0.57.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D2n5C0JSris"
      },
      "outputs": [],
      "source": [
        "from math import factorial\n",
        "\n",
        "def calculate_betweenness_centrality(social_network):\n",
        "    #TODO\n",
        "    n = social_network.number_of_nodes()\n",
        "    for i in social_network.nodes():\n",
        "        sumf_i = 0\n",
        "        nodes = list(social_network.nodes)\n",
        "        for j in nodes:\n",
        "            if j != i:\n",
        "                for k in nodes[nodes.index(j):]:\n",
        "                    if k != i:\n",
        "                        try:\n",
        "                            p = nx.shortest_path(social_network, source = j, target = k)\n",
        "                        except nx.NetworkXNoPath:\n",
        "                            p = []\n",
        "                        if i in p: \n",
        "                            sumf_i += 1\n",
        "        # nCr = (n!)/(r!(n-r)!)\n",
        "        r = 2\n",
        "        nCr = factorial(n) / (factorial(r)*factorial(n-r))\n",
        "        social_network.nodes[i]['betweenness_centrality'] = sumf_i / nCr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGVYdrAOSriu",
        "outputId": "ce7681d0-6a27-4158-987c-8225c756d376"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Highest betweenness centrality node: node_4 with 0.5735294117647058\n"
          ]
        }
      ],
      "source": [
        "def test_betweenness_centrality():\n",
        "    social_network = load_unweighted_network(\"data/toy-nodes2.csv\", \"data/toy-edges2.csv\")\n",
        "    calculate_betweenness_centrality(social_network)\n",
        "    dict_centrality = nx.get_node_attributes(social_network, 'betweenness_centrality')\n",
        "    best_node = max(dict_centrality, key=dict_centrality.get)\n",
        "    print(\"Highest betweenness centrality node: {} with {}\".format(best_node, dict_centrality[best_node]))\n",
        "\n",
        "test_betweenness_centrality()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "996_M0-sXG0w"
      },
      "source": [
        "## 2.4 Analyse de vos r√©sultats / Analysis of your results (2 points)\n",
        "\n",
        "Executez les trois fonctions sur les r√©seaux de chaque saison et pr√©sentez le top 10 pour chaque mesure. **Pour les saisons 2, 4 et 6 ne calculez pas la centralit√© de proximit√©**, car ce sont des graphes d√©connect√©s. Pour chaque saison, comparez le top 10 des mesures avec la liste de morts de la saison disponible dans les csv death. R√©pondez aux questions suivantes. Elles sont des pistes de r√©flexions pour votre analyse.\n",
        "\n",
        "- Est-ce que le top 10 est suffisant pour trouver les morts importants de chaque saison? \n",
        "- Quelle mesure semble mieux pr√©dire les morts? \n",
        "- Est-ce que la r√©putation de Games of Thrones de tuer plusieurs de ses personnages importants est fond√©e?\n",
        "\n",
        "**N.B.:** Si vous ne connaissez pas la s√©rie et vous n'√™tes pas s√ªrs quels morts peuvent √™tre consid√©r√©s importants, faites une recherche Google sur les personnages importants. Mentionnez votre d√©marche et la conclusion de vos recherches. Il n'y a pas une liste pr√©cise de morts importants. √âvidemment si vous me dite que Daenerys n'est pas importante, je vais douter de vos recherches. Le but est de voir votre travail de r√©flexion et d'analyse des mesures de centralit√©. \n",
        "\n",
        "---\n",
        "\n",
        "Run the three functions on the networks of each season and present the top 10 for each metric. **For season 2, 4 and 6 do not calculate the proximity centrality** because they are disconnected graph. For each season, compare the top 10 metrics with the season's death list in the death csv. Answer the following questions. They are guide for your analysis.\n",
        "\n",
        "- Is the top 10 enough to find the significant deaths of each season? \n",
        "- What measure seems to better predict the dead? \n",
        "- Is the reputation of Games of Thrones for killing many important characters founded?\n",
        "\n",
        "**N.B:** If you don't know the series and aren't sure which deaths are considered important, do a Google research on the important characters in the series. Metion your research and the conclusion of it. There isn't a precise list of important deaths but if you tell me that Daenerys isn't important, I will doubt of the seriousness of your research. The goal is to see how your analyse the results giving by centrality metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_S1YvI0Sriy"
      },
      "source": [
        "### R√©sultats / Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtYPFnJ3Sri2",
        "outputId": "d7b2cd7d-e7ec-4dbb-b928-bb761b490daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------Season 1------------------ \n",
            "\n",
            "--- Degree centrality ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Degree centrality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NED</th>\n",
              "      <td>0.456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TYRION</th>\n",
              "      <td>0.328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CATELYN</th>\n",
              "      <td>0.288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ROBERT</th>\n",
              "      <td>0.288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ROBB</th>\n",
              "      <td>0.240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CERSEI</th>\n",
              "      <td>0.232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ARYA</th>\n",
              "      <td>0.224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>JOFFREY</th>\n",
              "      <td>0.216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>JON</th>\n",
              "      <td>0.208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LITTLEFINGER</th>\n",
              "      <td>0.208</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Degree centrality\n",
              "NED                       0.456\n",
              "TYRION                    0.328\n",
              "CATELYN                   0.288\n",
              "ROBERT                    0.288\n",
              "ROBB                      0.240\n",
              "CERSEI                    0.232\n",
              "ARYA                      0.224\n",
              "JOFFREY                   0.216\n",
              "JON                       0.208\n",
              "LITTLEFINGER              0.208"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Proximity centrality ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Proximity centrality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NED</th>\n",
              "      <td>0.628141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ROBERT</th>\n",
              "      <td>0.553097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CATELYN</th>\n",
              "      <td>0.550661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TYRION</th>\n",
              "      <td>0.543478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>JON</th>\n",
              "      <td>0.518672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ROBB</th>\n",
              "      <td>0.512295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>JOFFREY</th>\n",
              "      <td>0.510204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CERSEI</th>\n",
              "      <td>0.508130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ARYA</th>\n",
              "      <td>0.502008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>JAIME</th>\n",
              "      <td>0.498008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Proximity centrality\n",
              "NED                  0.628141\n",
              "ROBERT               0.553097\n",
              "CATELYN              0.550661\n",
              "TYRION               0.543478\n",
              "JON                  0.518672\n",
              "ROBB                 0.512295\n",
              "JOFFREY              0.510204\n",
              "CERSEI               0.508130\n",
              "ARYA                 0.502008\n",
              "JAIME                0.498008"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Betweenness centrality ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Betweenness centrality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NED</th>\n",
              "      <td>0.381968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TYRION</th>\n",
              "      <td>0.151365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CATELYN</th>\n",
              "      <td>0.140825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DAENERYS</th>\n",
              "      <td>0.131683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>JON</th>\n",
              "      <td>0.105905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ROBERT</th>\n",
              "      <td>0.097778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ARYA</th>\n",
              "      <td>0.061460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BRAN</th>\n",
              "      <td>0.055238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ROBB</th>\n",
              "      <td>0.051302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PYP</th>\n",
              "      <td>0.049270</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Betweenness centrality\n",
              "NED                     0.381968\n",
              "TYRION                  0.151365\n",
              "CATELYN                 0.140825\n",
              "DAENERYS                0.131683\n",
              "JON                     0.105905\n",
              "ROBERT                  0.097778\n",
              "ARYA                    0.061460\n",
              "BRAN                    0.055238\n",
              "ROBB                    0.051302\n",
              "PYP                     0.049270"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Deaths ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Community</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BENJEN</td>\n",
              "      <td>Night Watch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DROGO</td>\n",
              "      <td>Dothraki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GARED</td>\n",
              "      <td>Doomed Night Watch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HUGH_OF_THE_VALE</td>\n",
              "      <td>King's Landing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>JON_ARRYN</td>\n",
              "      <td>King's Landing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>JORY_CASSEL</td>\n",
              "      <td>King's Landing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>MAGO</td>\n",
              "      <td>Dothraki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MIRRI_MAZ_DUUR</td>\n",
              "      <td>Dothraki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MYCAH</td>\n",
              "      <td>King's Landing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NED</td>\n",
              "      <td>King's Landing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>QOTHO</td>\n",
              "      <td>Dothraki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>RHAEGO</td>\n",
              "      <td>Dothraki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ROBERT</td>\n",
              "      <td>King's Landing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>SEPTA_MORDANE</td>\n",
              "      <td>King's Landing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>STABLE_BOY</td>\n",
              "      <td>King's Landing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SYRIO_FOREL</td>\n",
              "      <td>King's Landing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>VARDIS_EGEN</td>\n",
              "      <td>Lannister</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>VARLY</td>\n",
              "      <td>King's Landing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>VISERYS</td>\n",
              "      <td>Dothraki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>WAYMAR_ROYCE</td>\n",
              "      <td>Doomed Night Watch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>WILL</td>\n",
              "      <td>Doomed Night Watch</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Id           Community\n",
              "0             BENJEN         Night Watch\n",
              "1              DROGO            Dothraki\n",
              "2              GARED  Doomed Night Watch\n",
              "3   HUGH_OF_THE_VALE      King's Landing\n",
              "4          JON_ARRYN      King's Landing\n",
              "5        JORY_CASSEL      King's Landing\n",
              "6               MAGO            Dothraki\n",
              "7     MIRRI_MAZ_DUUR            Dothraki\n",
              "8              MYCAH      King's Landing\n",
              "9                NED      King's Landing\n",
              "10             QOTHO            Dothraki\n",
              "11            RHAEGO            Dothraki\n",
              "12            ROBERT      King's Landing\n",
              "13     SEPTA_MORDANE      King's Landing\n",
              "14        STABLE_BOY      King's Landing\n",
              "15       SYRIO_FOREL      King's Landing\n",
              "16       VARDIS_EGEN           Lannister\n",
              "17             VARLY      King's Landing\n",
              "18           VISERYS            Dothraki\n",
              "19      WAYMAR_ROYCE  Doomed Night Watch\n",
              "20              WILL  Doomed Night Watch"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "2\n",
            "2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nR√©pondez aux questions suivantes. Elles sont des pistes de r√©flexions pour votre analyse.\\n- Est-ce que le top 10 est suffisant pour trouver les morts importants de chaque saison?\\n- Quelle mesure semble mieux pr√©dire les morts?\\n- Est-ce que la r√©putation de Games of Thrones de tuer plusieurs de ses personnages importants est fond√©e?\\n'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "# Mettez le code pour pr√©senter les r√©sultats ici\n",
        "\"\"\"\n",
        "Executez les trois fonctions sur les r√©seaux de chaque saison et pr√©sentez le top 10 pour chaque mesure. \n",
        "Pour les saisons 2, 4 et 6 ne calculez pas la centralit√© de proximit√©, car ce sont des graphes d√©connect√©s. \n",
        "Pour chaque saison, comparez le top 10 des mesures avec la liste de morts de la saison disponible dans les csv death. \n",
        "\"\"\"\n",
        "\n",
        "ratio_degree_centrality = 0\n",
        "ratio_closeness_centrality = 0\n",
        "ratio_betweenness_centrality = 0\n",
        "\n",
        "# 1-8 saisons de got\n",
        "for season in range(1,2):\n",
        "    print(\"--------------------Season {}------------------ \\n\".format(season))\n",
        "    got_nodes = \"data/got-s{}-nodes.csv\".format(season)\n",
        "    got_edges = \"data/got-s{}-edges.csv\".format(season)\n",
        "    social_network = load_unweighted_network(got_nodes, got_edges)\n",
        "    \n",
        "    # Deaths\n",
        "    print(\"--- Deaths ---\")\n",
        "    got_deaths = \"data/got-s{}-deaths.csv\".format(season)\n",
        "    deaths = pd.read_csv(got_deaths)  \n",
        "    df_deaths = deaths[['Id','Community']]\n",
        "    display(df_deaths)\n",
        "    \n",
        "    # Degree centrality\n",
        "    print(\"--- Degree centrality ---\")\n",
        "    calculate_degree_centrality(social_network)\n",
        "    dict_centrality = nx.get_node_attributes(social_network, 'degree_centrality')\n",
        "    best_nodes = {}\n",
        "    for i in range(10):\n",
        "        best_node = max(dict_centrality, key=dict_centrality.get)\n",
        "        best_nodes[best_node] = dict_centrality[best_node]\n",
        "        dict_centrality.pop(best_node)\n",
        "\n",
        "    df_degree_centrality = pd.DataFrame.from_dict(best_nodes, orient='index', columns=['Degree centrality'])\n",
        "    display(df_degree_centrality)\n",
        "    \n",
        "    count_degree_centrality = 0\n",
        "    for i in range(df_degree_centrality.shape[0]):\n",
        "        if list(df_degree_centrality.index)[i] in set(deaths['Id']):\n",
        "            count_degree_centrality += 1\n",
        "    ratio_degree_centrality += count_degree_centrality/deaths.shape[0]\n",
        "            \n",
        "    # Proximity centrality\n",
        "    print(\"--- Proximity centrality ---\")\n",
        "        # X: 2, 4, 6, 8?\n",
        "    if season != 2 or season != 4 or season != 6 or season != 8:\n",
        "        calculate_closeness_centrality(social_network)\n",
        "        dict_centrality = nx.get_node_attributes(social_network, 'closeness_centrality')\n",
        "        best_nodes = {}\n",
        "        for i in range(10):\n",
        "            best_node = max(dict_centrality, key=dict_centrality.get)\n",
        "            best_nodes[best_node] = dict_centrality[best_node]\n",
        "            dict_centrality.pop(best_node)\n",
        "\n",
        "        df_closeness_centrality = pd.DataFrame.from_dict(best_nodes, orient='index', columns=['Proximity centrality'])\n",
        "        display(df_closeness_centrality)\n",
        "        \n",
        "        count_closeness_centrality = 0\n",
        "        for i in range(df_closeness_centrality.shape[0]):\n",
        "            if list(df_closeness_centrality.index)[i] in set(deaths['Id']):\n",
        "                count_closeness_centrality += 1\n",
        "        ratio_closeness_centrality += count_closeness_centrality/deaths.shape[0]\n",
        "        \n",
        "    # Betweenness centrality\n",
        "    print(\"--- Betweenness centrality ---\")\n",
        "    calculate_betweenness_centrality(social_network)\n",
        "    dict_centrality = nx.get_node_attributes(social_network, 'betweenness_centrality')\n",
        "    best_nodes = {}\n",
        "    for i in range(10):\n",
        "        best_node = max(dict_centrality, key=dict_centrality.get)\n",
        "        best_nodes[best_node] = dict_centrality[best_node]\n",
        "        dict_centrality.pop(best_node)\n",
        "\n",
        "    df_betweenness_centrality = pd.DataFrame.from_dict(best_nodes, orient='index', columns=['Betweenness centrality'])\n",
        "    display(df_betweenness_centrality)\n",
        "    \n",
        "    count_betweenness_centrality = 0\n",
        "    for i in range(df_betweenness_centrality.shape[0]):\n",
        "        if list(df_betweenness_centrality.index)[i] in set(deaths['Id']):\n",
        "            count_betweenness_centrality += 1\n",
        "    ratio_betweenness_centrality += count_betweenness_centrality/deaths.shape[0]\n",
        "    \n",
        "print(count_degree_centrality)\n",
        "print(count_betweenness_centrality)\n",
        "print(count_degree_centrality)\n",
        "\"\"\"\n",
        "R√©pondez aux questions suivantes. Elles sont des pistes de r√©flexions pour votre analyse.\n",
        "- Est-ce que le top 10 est suffisant pour trouver les morts importants de chaque saison?\n",
        "- Quelle mesure semble mieux pr√©dire les morts?\n",
        "- Est-ce que la r√©putation de Games of Thrones de tuer plusieurs de ses personnages importants est fond√©e?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeeyaJnKnDGB"
      },
      "source": [
        "### Analyse / Analysis\n",
        "\n",
        "2.4.1. Est-ce que le top 10 est suffisant pour trouver les morts importants de chaque saison?\n",
        "\n",
        "2.4.2. Quelle mesure semble mieux pr√©dire les morts?\n",
        "\n",
        "2.4.3. Est-ce que la r√©putation de Games of Thrones de tuer plusieurs de ses personnages importants est fond√©e?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi6efh01UjEU"
      },
      "source": [
        "# 3. DeepWalk (6 points)\n",
        "\n",
        "##  Pr√©diction de lien \n",
        "Une autre analyse int√©ressante √† faire avec un r√©seau social est la pr√©diction de liens. En regardant les liens actuels, il est possible de d√©terminer la probabilit√© qu'un lien apparaisse entre deux personnes dans le r√©seau. Pour ce TP, on vous demande d'impl√©menter le mod√®le de DeepWalk pour transformer les informations contenues dans le r√©seau et utiliser les **embeddings** pour pr√©dire des liens.\n",
        "\n",
        "## DeepWalk\n",
        "Le mod√®le de DeepWalk est bas√© sur l'id√©e de traiter un r√©seau social comme un texte. On peut ainsi utiliser des techniques d'apprentissage machine pour du traitement de language. C'est un mod√®le en deux √©tapes. La premi√®re √©tape est de contruire le \"dictionnaire\" du r√©seau en explorant le voisinage de chaque sommet. La deuxi√®me √©tape applique l'algorithme SkipGram sur le \"dictionnaire\" pour apprendre les *embeddings* pertinents. Le r√©seau est maintenant transform√© et pr√™t √† √™tre trait√© par des techniques de NLP.\n",
        "\n",
        "## GOT datasets\n",
        "Nous vous demandons de choisir une saison de GoT excluant les **saisons 2, 4 et 6** pour tester votre impl√©mentation du mod√®le. Enlever un lien fort, un lien moyen et un lien faible d'un sommet du r√©seau. Nous voulons voir si le mod√®le est capable de retourver ces 3 liens pour le sommet.\n",
        "\n",
        "---\n",
        "##  Link prediction\n",
        "Another interesting graph manipulation is link prediction. By looking at current links between nodes, the probability of connecting two nodes can be predicted. In this part of the TP, we want you to implement the DeepWalk model to transform the information in the graph to be used with NLP models to predict links.\n",
        "\n",
        "## DeepWalk\n",
        "The DeepWalk model is based on the idea that social network can be treated like a text. As such, NLP techniques can be used to mine a social network. It is a 2 step model. The first step builds the network's dictionnary by exploring the network. The second step use the SkipGram algorithm to transform the dictionnary in embeddings. The network is then ready to be used by NLP techniques.\n",
        "\n",
        "## GOT datasets\n",
        "We ask you to choose a GOT season excluding **season 2, 4 and 6** to test your implementation. Remove a strong, medium and low link from one node. We want to see if the model can find those links again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCVIF3ngSeby"
      },
      "outputs": [],
      "source": [
        "import random as rnd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import math\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "\n",
        "class DeepWalk:\n",
        "    def __init__(self, graph):\n",
        "        self.graph = graph\n",
        "\n",
        "        self.random_walks = []\n",
        "        self.walk_length = None\n",
        "        self.n_walk = None\n",
        "\n",
        "        self.node_index = None\n",
        "        self.training_data = None\n",
        "\n",
        "        self.n_neurons = None\n",
        "        self.epochs = None\n",
        "        self.embeddings = None\n",
        "        self.W1 = None\n",
        "        self.loss = 0\n",
        "        self.alpha = 0.00001\n",
        "        self.init = None\n",
        "\n",
        "    def random_walk(self, start_node):\n",
        "        # TODO\n",
        "        \"\"\"\n",
        "        Impl√©menter la fonction random_walk. \n",
        "        Cette fonction retourne une marche al√©atoire √† partir du sommet donn√© en param√®tre. \n",
        "        N.B: consid√©rer que les attributs self.n_walk et self.walk_length ont d√©j√† √©t√© initialis√© avec les bonnes valeurs.\n",
        "        \n",
        "        \"\"\"\n",
        "        random_walks = [start_node] # string\n",
        "        target_node = start_node\n",
        "        while(len(random_walks) < self.walk_length):\n",
        "            neighbors = list(nx.all_neighbors(self.graph, target_node))\n",
        "            if(len(neighbors) == 0):\n",
        "                break\n",
        "            target_node = random.choice(neighbors)\n",
        "            random_walks.append(target_node)\n",
        "        return random_walk\n",
        "        \n",
        "        \n",
        "    # n_walk int\n",
        "    def build_random_walks_matrix(self, n_walk, walk_length):\n",
        "        # TODO\n",
        "        \"\"\"\n",
        "        Impl√©menter la fonction build_random_walks_matrix. \n",
        "        Cette fonction trouve le voisinage de tous les sommets du r√©seau. \n",
        "        Cela g√©n√®re donc une matrice de taille (nbre_sommet * n_walk) x walk_length, \n",
        "            car chaque sommet a n_walk marches al√©atoires. \n",
        "        Cette matrice est contenu dans self.random_walks.\n",
        "        \"\"\"\n",
        "        self.walk_length = walk_length\n",
        "        self.n_walk = n_walk\n",
        "        random_walks = []\n",
        "        for node in self.graph.nodes:\n",
        "            for path_count in range(self.n_walk):\n",
        "                random_walks.append(self.random_walk(node))\n",
        "        return random_walks\n",
        "\n",
        "    def create_training(self, window_size):\n",
        "        # TODO\n",
        "        \"\"\"\n",
        "        ?\n",
        "        \"\"\"\n",
        "\n",
        "    def neural_network(self, epochs, n_neurons):\n",
        "        self.n_neurons = n_neurons\n",
        "        self.epochs = epochs\n",
        "        self.init = math.sqrt(6 /\n",
        "                              (self.graph.number_of_nodes() + self.n_neurons))\n",
        "        self.embeddings = np.random.uniform(\n",
        "            -self.init, self.init,\n",
        "            (self.graph.number_of_nodes(), self.n_neurons))\n",
        "        self.W1 = np.random.uniform(\n",
        "            -self.init, self.init,\n",
        "            (self.n_neurons, self.graph.number_of_nodes()))\n",
        "\n",
        "        for x in range(1, epochs):\n",
        "            self.loss = []\n",
        "            for x_data, y_data in self.training_data:\n",
        "                self._feed_forward(x_data)\n",
        "                self._backpropagate(x_data, y_data)\n",
        "                C = 0\n",
        "                sub_loss = 0\n",
        "                for m in range(self.graph.number_of_nodes()):\n",
        "                    if y_data[m]:\n",
        "                        sub_loss += -1 * self.u[m][0]\n",
        "                        C += 1\n",
        "                sub_loss += C * np.log(np.sum(np.exp(self.u)))\n",
        "                self.loss.append(sub_loss)\n",
        "            self.loss = np.mean(self.loss)\n",
        "            print(\"epoch \", x, \" loss = \", self.loss)\n",
        "            self.alpha *= 1 / (1 + self.alpha * x)\n",
        "\n",
        "    def skip_gram(self, window_size, epochs, n_neurons):\n",
        "        self.create_training(window_size)\n",
        "        self.neural_network(epochs, n_neurons)\n",
        "        return self.embeddings, self.W1\n",
        "\n",
        "    def _feed_forward(self, X):\n",
        "        self.h = np.dot(self.embeddings.T, X).reshape(self.n_neurons, 1)\n",
        "        self.u = np.dot(self.W1.T, self.h)\n",
        "        self.y = softmax(self.u)\n",
        "        return self.y\n",
        "\n",
        "    def _backpropagate(self, x, t):\n",
        "        # e is a vector V x 1\n",
        "        e = self.y - np.asarray(t).reshape(self.graph.number_of_nodes(), 1)\n",
        "        dLdW1 = np.dot(self.h, e.T)\n",
        "        X = np.array(x).reshape(self.graph.number_of_nodes(), 1)\n",
        "        dLdW = np.dot(X, np.dot(self.W1, e).T)\n",
        "        self.W1 = self.W1 - self.alpha * dLdW1\n",
        "        self.embeddings = self.embeddings - self.alpha * dLdW\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZoPSKxISQar"
      },
      "source": [
        "## 3.1 Random-walk (2 points)\n",
        " \n",
        "La premi√®re √©tape du mod√®le DeepWalk consiste √† parcourir de mani√®re al√©atoire le r√©seau pour se construire un \"dictionnaire\" du r√©seau. La marche al√©atoire d√©pend de deux param√®tres: le nombre de marche(n_walk) et la longueur de la marche(walk_length). La longeur de la marche correspond au nombre de sommets dans la marche. En explorant le graphe, le mod√®le se construit des \"contextes\" √† partir des liens entres les sommets. On peut consid√©rer cela comme des phrases ou des extraits de phrases dans un texte. Deux sommets sont similaires si leur contexte le sont aussi.\n",
        " \n",
        " \n",
        "Impl√©mentation\n",
        "1. Impl√©menter la fonction `random_walk`. Cette fonction retourne une marche al√©atoire √† partir du sommet donn√© en param√®tre. **N.B:** consid√©rer que les attributs `self.n_walk` et `self.walk_length` ont d√©j√† √©t√© initialis√© avec les bonnes valeurs.\n",
        "2. Impl√©menter la fonction `build_random_walks_matrix`. Cette fonction trouve le voisinage de tous les sommets du r√©seau. Cela g√©n√®re donc une matrice de taille (nbre_sommet * n_walk) x walk_length, car chaque sommet a n_walk marches al√©atoires. Cette matrice est contenu dans `self.random_walks`.\n",
        "\n",
        "Pour augmenter la composante al√©atoire de la matrice, l'ordre des sommets doit aussi √™tre al√©atoire. Il ne faut pas que les n_walk premi√®res rang√©es contiennent toutes les marches pour le m√™me sommet du graphe ou que les marches suivent toujours le m√™me ordre de sommets. \n",
        "\n",
        "---\n",
        "The first step in the DeepWalk model is to create the dictionary by exploring the network randomly. The random walk depends on two parameters: the number of walks(`n_walk`) the length of the walk(`walk_length`). The length of the walk is the number of node in the walk. By exploring the graph, the model builds context with the links between nodes. Those links make up \"sentences\". Nodes are similar if their context are similar.\n",
        "\n",
        "Implementation\n",
        "\n",
        "1. Implement the function `random_walk`. This function returns a random walk starting with the node `start_node`. N.B: consider that self.n_walk and self.walk_length are already initialize with the right values.\n",
        "2. Implement the function `build_random_walks_matrix`. This function finds the neighborhood for all the nodes in the network. This generates a (`n_nodes` * `n_walk`) x `walk_length` matrix. Each node has n_walk random walks. The matrix is stored in `self.random_walks`.\n",
        "\n",
        "To ensure randomness in the matrix, the order of the nodes have to be random too. The n_walk first rows can't contains walk from the same node or that the walks always follow the same node order.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8DPJqFcsAtc"
      },
      "source": [
        "## 3.2 SkipGram (1 points)\n",
        " \n",
        "La deuxi√®me √©tape consiste √† transformer le \"dictionnaire\" en *embedding*.\n",
        "\n",
        "1. Commencer par cr√©er l'ensemble d'entrainement √† partir des marches al√©atoires cr√©es pr√©c√©demment en impl√©mentant la fonction `create_training`. Pour cr√©er l'ensemble, il faut it√©rer √† travers chaque sommet de chaque marche. Le sommet actuel est consid√©r√© comme le target. Il servira d'input. Son contexte/voisinage servira √† v√©rifier la pr√©diction. Ce contexte correspond au `window_size` sommets avant et apr√®s le sommet actuel dans la marche. Ces paires de target/contexte sont conserv√©s dans `self.training_data`.  Chaque  ligne contient un target et son contexte. Pour faciliter l'utilisation de ses informations un encoding one-hot est utilis√©. Le target est donc un vecteur avec un 1 √† l'index du sommet actuel. Le contexte est donc un vecteur avec des 1+(au cas o√π un sommet se retrouve plusieurs fois dans le voisinage) √† l'index des sommets voisins. Le vocabulaire consid√©r√© pour l'encoding est l'ensemble des sommets dans le r√©seau. \n",
        "\n",
        "La fonction `skip_gram` est impl√©ment√©e pour vous. Cette fonction utilise l'ensemble d'entrainement pour trouver les embedding des sommets. Pour faire cela, le mod√®le envoie chaque target dans un r√©seau de neurones √† une couche et fait un sofmax sur le r√©sultat pour comparer le r√©sultat avec le contexte. Le r√©seau de neurone est impl√©ment√© dans la fonction `neural_network`.\n",
        "\n",
        "---\n",
        "\n",
        "The second step is to transform the dictionary in embedding.\n",
        "\n",
        "1. Start by creating a training set from the random walks created previously in the function `create_training_set`. Each node in each walk is considered as a target. The context/neighborhood is the `window_size` nodes before and after the target. The target is used as an input while the context is used as the groundtruth for the prediction. Those pairs of target/context are stored in `self.training_data`. Each row has the target followed by its context. That information is encoded with one-hot encoding. The target is then a vector with a 1 in the index of the actual node. The context is a vector with 1+ (for cases where a node appears multiple times in the neighborhood) in the index of the neighboring nodes. The vocabulary used for the encoding is all the nodes in the network.\n",
        "\n",
        "The function `skip_gram` is implemented for you. This function uses the training set to find the nodes' embedding. The model gives each target to the neural network and uses a softmax on the results to compare it with the context. The model then backpropagate the error to correct the embedding. The neural network is in the fonction `neural_network`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVihLggbF-4V"
      },
      "source": [
        "## 3.3 Pr√©diction de liens / Link prediction (3 points)\n",
        " \n",
        "La derni√®re √©tape consiste √† utiliser les *embedding* pour faire de la pr√©diction de lien.\n",
        "\n",
        "Nous vous demandons de choisir une saison de GoT excluant les **saisons 2, 4 et 6** pour tester votre impl√©mentation du mod√®le. Enlevez un lien fort, un lien moyen et un lien faible d'un sommet du r√©seau. Utilisez la valeur de `weight` dans le csv de edge pour d√©terminer la force du lien. Apr√®s avoir enlev√© les 3 liens, appliquez DeepWalk sur le graph r√©sultant et utilisez les *embeddings* pour pr√©dire des liens sur le sommet. Nous voulons voir si le mod√®le est capable de retourver ces 3 liens pour le sommet. Les embeddings sont dans self.embeddings. Chaque rang√©e correspond aux embeddings pour un sommet dans le r√©seau. Votre mod√®le devra prendre deux vecteurs et pr√©dire 1 s'il y a un lien et 0 s'il n'y en a pas. **Indice**: deux vectors proches ont plus de chance d'√™tre li√©s.\n",
        "\n",
        "Montrez et discutez des r√©sultats obtenus. R√©pondez aux questions suivantes. Elles vous serviront de guides pour votre r√©flexion.\n",
        "\n",
        "- Expliquez votre mod√®le et pourquoi vous l'avez choisi.\n",
        "- Avez-vous r√©ussi √† trouver les 3 liens facilement?\n",
        "- Trouvez-vous des liens non-existants dans le graph de la saison actuelle?\n",
        "- Quels sont les impacts des param√®tres sur vos r√©sultats?\n",
        "\n",
        "---\n",
        "\n",
        "The last step is to use the embeddings to predict new links.\n",
        "\n",
        "We ask you to choose a GoT season **excluding the season 2, 4 and 6** to test your implementation. Remove a strong, medium and weak link from a node in the network. Use the `weight` value in the edge csv to check the links' strength. After removing the 3 links, apply the DeepWalk model on the resulting network and try to predict those 3 links. The embeddings are stored in self.embedding. Each row is the embedding for a node in the network. Your  model should take two vectors and output 1 for a link and 0 if not. **Hint**: linked vectors would be close with eact other.\n",
        "\n",
        "Show and discuss your result. Answer the following questions. They are guides for your reflexion.\n",
        "\n",
        "- Explain your model and why you chose it.\n",
        "- Did you find all 3 links?\n",
        "- Did you find links present in other seasons but not in this one?\n",
        "- How did the parameter impacts your results?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0p1nqLgpnw9"
      },
      "source": [
        "### R√©sultats / Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLLU3d5qGhSg"
      },
      "outputs": [],
      "source": [
        "# code pour votre pr√©diction ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eG-a3IlnI0G"
      },
      "source": [
        "### Analyse / Analysis\n",
        "√âcrivez votre analyse ici"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}